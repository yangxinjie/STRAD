{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.configdefaults): install mkl with `conda install mkl-service`: No module named 'mkl'\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "#import sys\n",
    "import numpy\n",
    "import numpy as np\n",
    "import downhill\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "\n",
    "from hiddenLayer import HiddenLayer\n",
    "from hiddenLayer import dA\n",
    "import pandas as pd\n",
    "import numpy\n",
    "from ProcessingData import load_data, normalize_data\n",
    "from Methods import auc_density, auc_AEbased\n",
    "from Plot_Curves import Plotting_End2End_RE, Plotting_Pre_RE, Plotting_AUC_RE\n",
    "from Plot_Curves import Plotting_AUC_Batch_Size, Plotting_Monitor, plot_auc_size_input, visualize_hidden1\n",
    "from Plot_Curves import Plotting_Loss_Component, plot_auc_size_2\n",
    "from nnet_architecture import hyper_parameters\n",
    "from stopping_para import stopping_para_shrink\n",
    "#import timeit as tm\n",
    "\n",
    "path = \"./Results/\"\n",
    "\n",
    "#Check whether weights matrix is updated or not\n",
    "def check_weight_update(sda):\n",
    "    np.set_printoptions(precision=4, suppress=True)\n",
    "    \"Check whether weights matrix is updated or not\"\n",
    "    for i in range(sda.n_layers):\n",
    "        print(\"\\n %d\" %i)\n",
    "        print (sda.Autoencoder_layers[i].W.get_value(borrow=True))\n",
    "\n",
    "    for j in range(sda.n_layers, 2*sda.n_layers):\n",
    "        print(\"\\n %d\" % j)\n",
    "        print (sda.Autoencoder_layers[j].W.eval())\n",
    "\n",
    "\n",
    "\n",
    "class SdA(object):\n",
    "    def __init__(self, numpy_rng, theano_rng=None, n_ins=100,\n",
    "                 hidden_layers_sizes=[50, 30], corruption_levels=[0.1, 0.1]):\n",
    "\n",
    "        self.encoder = []\n",
    "        self.dA_layers = []\n",
    "        self.params = []\n",
    "        self.n_layers = len(hidden_layers_sizes)\n",
    "        self.decoder = []\n",
    "\n",
    "        assert self.n_layers > 0\n",
    "        if not theano_rng:\n",
    "            theano_rng = RandomStreams(numpy_rng.randint(2 ** 30))\n",
    "\n",
    "        # allocate symbolic variables for the data\n",
    "        self.x = T.matrix('x')  # the data is presented as rasterized images\n",
    "\n",
    "        \"*************** Encoder **************************\"\n",
    "        for i in range(self.n_layers):\n",
    "            # the size of the input is either the number of hidden units of\n",
    "            # the layer below or the input size if we are on the first layer\n",
    "            if i == 0:\n",
    "                input_size = n_ins\n",
    "                layer_input = self.x\n",
    "            else:\n",
    "                input_size = hidden_layers_sizes[i - 1]\n",
    "                layer_input = self.encoder[-1].output\n",
    "            act_function = T.tanh\n",
    "            # the input to this layer is either the activation of the hidden\n",
    "            # layer below or the input of the SdA if you are on the first layer\n",
    "            encoder_layer = HiddenLayer(rng=numpy_rng,\n",
    "                                    input       = layer_input,\n",
    "                                    n_in        = input_size,\n",
    "                                    n_out       = hidden_layers_sizes[i],\n",
    "                                    activation  = act_function)\n",
    "            self.encoder.append(encoder_layer)\n",
    "            self.params.extend(encoder_layer.params)\n",
    "\n",
    "            \"**************************************************\"\n",
    "            \"\"\"Construct a dAE that shared weights with this layer\"\"\"\n",
    "#             dA_layer = dA(numpy_rng = numpy_rng,\n",
    "#                           theano_rng = theano_rng,\n",
    "#                           #input = layer_input,                #if use early-stopping for pre-train, it is disable\n",
    "#                           n_visible = input_size,\n",
    "#                           n_hidden  = hidden_layers_sizes[i])\n",
    "#                           #W         = encoder_layer.W,\n",
    "#                           #bhid      = encoder_layer.b)             #bvis: will be create dA itself\n",
    "\n",
    "#             self.dA_layers.append(dA_layer)\n",
    "            #dA will not initialize wieghts and bias\n",
    "\n",
    "        \"*************** Decoder *****************************\"\n",
    "        i = self.n_layers-1\n",
    "        while (i >=0):\n",
    "            input_size = hidden_layers_sizes[i]\n",
    "            if ( i > 0):\n",
    "                output_size = hidden_layers_sizes[i-1]\n",
    "            else:\n",
    "                output_size =  n_ins\n",
    "\n",
    "            if (i==self.n_layers-1):\n",
    "                layer_input = self.encoder[-1].output\n",
    "            else:\n",
    "                layer_input = self.decoder[-1].output\n",
    "\n",
    "            decoder_layer = HiddenLayer(rng     = numpy_rng,\n",
    "                                        input   = layer_input,\n",
    "                                        n_in    = input_size,\n",
    "                                        n_out   = output_size,\n",
    "                                        activation = T.tanh,\n",
    "                                        W = self.encoder[i].W.T)\n",
    "                                        #b = self.dA_layers[i].b_prime)    #this is bvis of dA\n",
    "            self.decoder.append(decoder_layer)\n",
    "            self.params.append(decoder_layer.b)\n",
    "            i = i - 1\n",
    "\n",
    "        \"******************* End To End Cost function ************************\"\n",
    "        y = self.encoder[-1].output\n",
    "        z = self.decoder[-1].output\n",
    "\n",
    "        lamda = 10.0\n",
    "        self.shrink = lamda*(((y)**2).mean(1)).mean()\n",
    "        self.recon = (((self.x - z)**2).mean(1)).mean()\n",
    "        self.end2end_cost = self.recon + self.shrink\n",
    "\n",
    "        #mean(1) is within example, mean(0) is within each feature\n",
    "\n",
    "    \"****** Error on train_x and valid_x before optimization process **********\"\n",
    "    def Loss_train_valid(self, train_x, valid_x):\n",
    "        index = T.lscalar('index')\n",
    "\n",
    "        train_size = train_x.get_value().shape[0]\n",
    "        tm = theano.function([index],\n",
    "                             outputs = self.end2end_cost,\n",
    "                             givens={self.x: train_x[index : train_size]})\n",
    "\n",
    "        valid_size = valid_x.get_value().shape[0]\n",
    "        vm = theano.function([index],\n",
    "                             outputs = self.end2end_cost,\n",
    "                             givens={self.x: valid_x[index : valid_size]})\n",
    "\n",
    "        return tm(0), vm(0)\n",
    "\n",
    "    \"****************** Compute recon and shrink component *******************\"\n",
    "#    def Loss_recon_shrink(self, train_x):\n",
    "#        index = T.lscalar('index')\n",
    "#        train_size = train_x.get_value().shape[0]\n",
    "#        loss_com = theano.function([index],\n",
    "#                             outputs = [self.recon, self.shrink],\n",
    "#                             givens={self.x: train_x[index : train_size]})\n",
    "#        return loss_com(0)\n",
    "\n",
    "\n",
    "\n",
    "    \"Compute loss for each batch\"\n",
    "    def Loss_recon_shrink_batch(self, train_x, batch_size):\n",
    "\n",
    "        index = T.lscalar('index')\n",
    "        # begining of a batch, given `index`\n",
    "        batch_begin = index * batch_size\n",
    "        # ending of a batch given `index`\n",
    "        batch_end = batch_begin + batch_size\n",
    "\n",
    "        loss_com = theano.function([index],\n",
    "                             outputs = [self.recon, self.shrink],\n",
    "                             givens={self.x: train_x[batch_begin : batch_end]})\n",
    "        return loss_com\n",
    "\n",
    "    \"Compute loss for n_batch from train_x\"\n",
    "    def Loss_recon_shrink(self, train_x, batch_size):\n",
    "        n_train = train_x.get_value().shape[0]\n",
    "        n_batches = (int)(n_train/batch_size)\n",
    "        loss_com = self.Loss_recon_shrink_batch(train_x, batch_size)\n",
    "        loss = np.empty([0,2])\n",
    "        for batch_index in range(n_batches):\n",
    "          l = loss_com(index = batch_index)\n",
    "          loss = np.append(loss, [l[0], l[1]])\n",
    "        loss = np.reshape(loss, (-1,2))\n",
    "\n",
    "        return (loss.mean(0))\n",
    "\n",
    "\n",
    "    \"Get data from the middle hidden layer Deep Autoencoder\"\n",
    "    def get_hidden_data(self,data_set):\n",
    "\n",
    "        data_size = data_set.get_value().shape[0]\n",
    "        index = T.lscalar('index')\n",
    "        hidden_data = theano.function([index],\n",
    "                                      outputs = self.encoder[-1].output,\n",
    "                                      givens={self.x: data_set[index : data_size]})\n",
    "        #Get hidden_data from Autoencoder is the same getting data from the last\n",
    "        return hidden_data(0)\n",
    "\n",
    "    #Get hidden data from hidden layer i-th for pre-training\n",
    "    def get_hidden_i(self,data_set, i):\n",
    "\n",
    "        data_size = data_set.get_value().shape[0]\n",
    "        index = T.lscalar('index')\n",
    "        hidden_data = theano.function([index],\n",
    "                                      outputs = self.encoder[i].output,\n",
    "                                      givens={self.x: data_set[index : data_size]})\n",
    "        #Get hidden_data from Autoencoder is the same getting data from the last\n",
    "        return hidden_data(0)\n",
    "\n",
    "    \"Get data from the output of Autoencoder\"\n",
    "    def get_output_data(self,data_set):\n",
    "\n",
    "        data_size = data_set.get_value().shape[0]\n",
    "        index = T.lscalar('index')\n",
    "        output_data = theano.function([index],\n",
    "                                      outputs = self.decoder[-1].output,\n",
    "                                      givens={self.x: data_set[index : data_size]})\n",
    "        return output_data(0)\n",
    "\n",
    "\n",
    "\n",
    "    def Compute_AUC_Hidden(self, train_set, test_set, actual, norm, data_name):\n",
    "\n",
    "        #output_test  = self.get_output_data(test_set)       #get prediction values\n",
    "        train_hidden = train_set#self.get_hidden_data(train_set)      #get hidden values\n",
    "        test_hidden  = test_set#self.get_hidden_data(test_set)       #get hidden values\n",
    "\n",
    "        \"Compute performance of classifiers on latent data\"\n",
    "        lof, cen, dis, kde, svm05, svm01 = auc_density(train_hidden, test_hidden, actual, norm)\n",
    "        #ae                               = auc_AEbased(test_set.get_value(), output_test, actual)\n",
    "        return lof, cen, dis, kde, svm05, svm01\n",
    "\n",
    "    \"**************************************************************************\"\n",
    "    def Save_Hidden_Data(self, train_set, test_set, data_name, path):\n",
    "\n",
    "        train_hidden = self.get_hidden_data(train_set)      #get hidden values\n",
    "        test_hidden  = self.get_hidden_data(test_set)       #get hidden values\n",
    "        np.savetxt(path + data_name + \"_train_z.csv\", train_hidden, delimiter=\",\", fmt='%f' )\n",
    "        np.savetxt(path + data_name + \"_test_z.csv\", test_hidden, delimiter=\",\", fmt='%f' )\n",
    "\n",
    "    \"**************************************************************************\"\n",
    "    def Save_Hidden_Data_Size(self, train_set, test_set, data_name, size, path):\n",
    "\n",
    "        train_hidden = self.get_hidden_data(train_set)      #get hidden values\n",
    "        test_hidden  = self.get_hidden_data(test_set)       #get hidden values\n",
    "        np.savetxt(path + \"data/\" + data_name + \"_train_z_\" + str(size) + \".csv\", train_hidden, delimiter=\",\", fmt='%f' )\n",
    "        np.savetxt(path + \"data/\"+ data_name + \"_test_z_\" + str(size) +\".csv\", test_hidden, delimiter=\",\", fmt='%f' )\n",
    "\n",
    "    \"******** Training End-to-End Early-stopping by Downhill Package *********\"\n",
    "    def End2end_Early_stopping(self, numpy_rng, dataset, n_validate, data_name,\n",
    "                               batch_size, end2end_lr, algo, norm, patience, validation):\n",
    "\n",
    "        train_X, test_X, actual = dataset\n",
    "        valid_x = train_X.get_value()[:n_validate]\n",
    "        train_x = train_X.get_value()[n_validate:]\n",
    "\n",
    "        \"for compute tm and vm before optimization process\"\n",
    "        t = theano.shared(numpy.asarray(train_x, dtype=theano.config.floatX), borrow=True)\n",
    "        v = theano.shared(numpy.asarray(valid_x, dtype=theano.config.floatX), borrow=True)\n",
    "\n",
    "        \"Use downhill for training network\"\n",
    "        #'adadelta' 'adagrad (default 0.01)' 'adam''esgd' 'nag''rmsprop' 'rprop' 'sgd'\n",
    "        opt = downhill.build(algo = algo, params= self.params,\n",
    "                             loss = self.end2end_cost, inputs = [self.x])\n",
    "\n",
    "        train = downhill.Dataset(train_x, batch_size = batch_size, rng = numpy_rng)\n",
    "        valid = downhill.Dataset(valid_x, batch_size = len(valid_x), rng = numpy_rng)\n",
    "\n",
    "        \"for monitoring before optimization process\"\n",
    "        stop_ep = 0\n",
    "\n",
    "#        LOSS = np.empty([0,4])\n",
    "#        monitor = np.empty([0,8])\n",
    "                                       #performance before fine-tuning\n",
    "#        lof,cen,dis,kde,svm05,svm01,ae = self.Compute_AUC_Hidden(train_X, test_X, actual, norm, data_name)\n",
    "#        a = [stop_ep, lof, cen, dis, kde, svm05, svm01, ae]\n",
    "#        monitor = np.append(monitor, a )\n",
    "                                        #Loss component\n",
    "#        loss = self.Loss_recon_shrink(t, batch_size)\n",
    "#        LOSS = np.append(LOSS,[0.0, loss[0], loss[1], loss[0]+loss[1]])\n",
    "#                                        #error before training\n",
    "\n",
    "        for tm1, vm1 in opt.iterate(train,                        # 10, 5, 1e-2, 0.0\n",
    "                                  valid,\n",
    "                                  patience = patience,                # 10\n",
    "                                  validate_every= validation,            # 5\n",
    "                                  min_improvement = 1e-3,       # 1e-3\n",
    "                                  #learning_rate =  end2end_lr, # 1e-4\n",
    "                                  momentum = 0.0,\n",
    "                                  nesterov = False):\n",
    "\n",
    "\n",
    "            stop_ep = stop_ep + 1\n",
    "#            loss = self.Loss_recon_shrink(t, batch_size)\n",
    "#            LOSS = np.append(LOSS,[stop_ep, loss[0], loss[1], loss[0]+loss[1]])\n",
    "#\n",
    "##            \"******* Classification Results after End to End training ******\"\n",
    "#            if ((stop_ep%1 == 0) and (stop_ep > 0)):\n",
    "#                lof,cen,dis,kde,svm05,svm01,ae = self.Compute_AUC_Hidden(train_X, test_X, actual, norm, data_name)\n",
    "#                a = [stop_ep, lof, cen, dis, kde, svm05, svm01, ae]\n",
    "#            monitor = np.append(monitor, a)\n",
    "\n",
    "            if (stop_ep >= 1000):\n",
    "                break\n",
    "\n",
    "        #Plotting AUC and save to csv file\n",
    "#        monitor = np.reshape(monitor, (-1,8))\n",
    "#        Plotting_Monitor(monitor, 0.4, 1.0, data_name, path)\n",
    "#        np.savetxt(path + data_name + \"_monitor_auc.csv\", monitor, delimiter=\",\", fmt='%f' )\n",
    "\n",
    "\n",
    "#        LOSS = np.reshape(LOSS, (-1,4))\n",
    "#        Plotting_Loss_Component(LOSS, RE, 0.0, 0.1, data_name, path)\n",
    "#        np.savetxt(path + data_name + \"_loss_component.csv\", LOSS, delimiter=\",\", fmt='%f' )\n",
    "\n",
    "        return  [stop_ep, vm1['loss'], tm1['loss']]\n",
    "\n",
    "\n",
    "\n",
    "def test_SdA(pre_lr=0.01, end2end_lr=1e-4, algo = 'sgd',\n",
    "             dataset=[], data_name = \"WBC\", n_validate = 0, norm = \"maxabs\",\n",
    "             batch_size=10, hidden_sizes = [1,1,1], corruptions = [0.0, 0.0, 0.0],\n",
    "             patience = 1, validation = 1):\n",
    "\n",
    "    numpy_rng = numpy.random.RandomState(89677)   # numpy random generator 89677\n",
    "    train_X, test_X, actual = dataset             # dataset is already normalised\n",
    "\n",
    "    input_size = train_X.get_value().shape[1]     # input size = dimension\n",
    "    train_x    = train_X.get_value()[n_validate:]   # 80% for pre-training, 20% for validation\n",
    "    n_train_batches   = train_x.shape[0]\n",
    "    n_train_batches //= batch_size                  # number of batches for pre-training\n",
    "\n",
    "    # construct the stacked denoising autoencoder class\n",
    "    sda = SdA(numpy_rng = numpy_rng, n_ins = input_size,\n",
    "              hidden_layers_sizes = hidden_sizes)\n",
    "\n",
    "    #check_weight_update(sda)    #Check whether weights matrix is updated or not\n",
    "    RE = sda.End2end_Early_stopping(numpy_rng, dataset, n_validate, data_name,\n",
    "                               batch_size, end2end_lr, algo, norm, patience, validation)\n",
    "\n",
    "    return sda, RE\n",
    "\n",
    "\"********************************* Main experiment ***************************\"\n",
    "def Main_Test():\n",
    "\n",
    "    list_data = [\"PageBlocks\", \"WPBC\", \"PenDigits\", \"GLASS\", \"Shuttle\", \"Arrhythmia\",\\\n",
    "                 \"CTU13_10\", \"CTU13_08\",\"CTU13_09\",\"CTU13_13\",\\\n",
    "                 \"Spambase\", \"UNSW\", \"NSLKDD\", \"InternetAds\"]\n",
    "\n",
    "    norm         = \"maxabs\"                 #standard, maxabs[-1,1] or minmax[0,1]\n",
    "    corruptions  = [0.1, 0.1, 0.1]\n",
    "\n",
    "    print (\"Shrink - 10\")\n",
    "    print (\"+ Data: \", list_data)\n",
    "    print (\"+ Scaler: \", norm)\n",
    "    print (\"+ Corruptions: \", corruptions)\n",
    "\n",
    "    AUC_Hidden = np.empty([0,10])     #store auc of all hidden data\n",
    "    num = 0\n",
    "    for data in list_data:\n",
    "        num = num + 1\n",
    "        h_sizes = hyper_parameters(data)                   #Load hyper-parameters\n",
    "\n",
    "        train_set, test_set, actual = load_data(data)      #load original data\n",
    "        train_X, test_X = normalize_data(train_set, test_set, norm)  #Normalize data\n",
    "\n",
    "        train_X = theano.shared(numpy.asarray(train_X, dtype=theano.config.floatX), borrow=True)\n",
    "        test_X  = theano.shared(numpy.asarray(test_X,  dtype=theano.config.floatX), borrow=True)\n",
    "\n",
    "        datasets = [(train_X), (test_X), (actual)]          #Pack data for training AE\n",
    "\n",
    "        in_dim   = train_set.shape[1]                       #dimension of input data\n",
    "        n_vali   = (int)(train_set.shape[0]/5)              #size of validation set\n",
    "        n_train  = len(train_set) - n_vali                  #size of training set\n",
    "        #batch     = int(n_train/20)                          #Training set will be split training set into 20 batches\n",
    "\n",
    "        pat, val, batch, n_batch = stopping_para_shrink(n_train)\n",
    "\n",
    "\n",
    "        print (\"\\n\" + str(num) + \".\", data, \"...\" )\n",
    "        print (\" + Hidden Sizes: \",in_dim, h_sizes, \"- Batch_sizes:\", batch)\n",
    "        print (\" + Data: %d (%d train, %d vali) - %d normal, %d anomaly\"\\\n",
    "            %(len(train_set), n_train, n_vali, \\\n",
    "            len(test_set[(actual == 1)]), len(test_set[(actual == 0)])))\n",
    "        print(\" + Patience: %5.0d, Validate: %5.0d,  \\n + Batch size: %5.0d, n batch:%5.0d\"\\\n",
    "             %(pat, val, batch, n_batch))\n",
    "\n",
    "#        AUC_RE   = np.empty([0,10])\n",
    "\n",
    "                               #adadelta, 'adagrad' 'adam''esgd' 'nag''rmsprop' 'rprop' 'sgd'\n",
    "#        if (num==1):\n",
    "        sda, re = test_SdA(pre_lr       = 1e-2,              #re = [stop_ep, vm, tm]\n",
    "                            end2end_lr   = 1e-4,\n",
    "                            algo         = 'adadelta',\n",
    "                            dataset      = datasets,\n",
    "                            data_name    = data,\n",
    "                            n_validate   = n_vali,\n",
    "                            norm         = norm,\n",
    "                            batch_size   = batch,\n",
    "                            hidden_sizes = h_sizes,\n",
    "                            corruptions  = corruptions,\n",
    "                            patience     = pat,\n",
    "                            validation   = val)\n",
    "\n",
    "\n",
    "        #*******Computer AUC on hidden data*************\n",
    "        lof,cen,dis,kde,svm05,svm01,ae  = sda.Compute_AUC_Hidden(train_X, test_X, actual, norm, data)\n",
    "        auc_hidden = np.column_stack([batch, re[0], lof, cen, dis, kde, svm05, svm01, ae , 100*re[2]])\n",
    "        AUC_Hidden = np.append(AUC_Hidden, auc_hidden)\n",
    "\n",
    "        #save hidden data to files\n",
    "#        sda.Save_Hidden_Data(train_X, test_X, data, path)\n",
    "\n",
    "        #Display for saving to document\n",
    "#        AUC_RE   = np.append(AUC_RE, auc_hidden)\n",
    "#        AUC_RE   = np.reshape(AUC_RE,(-1,10))\n",
    "#\n",
    "#        np.set_printoptions(precision=3, suppress=True)\n",
    "#        column_list = [2,3,4,5,6,7,8,9]\n",
    "#        print (AUC_RE[:,column_list])\n",
    "\n",
    "    AUC_Hidden  =  np.reshape(AUC_Hidden, (-1, 10))\n",
    "    np.set_printoptions(precision=3, suppress=True)\n",
    "    column_list = [2,3,4,5,6,7,8,9]\n",
    "    print(\"    LOF    CEN    MDIS   KDE   SVM5    SVM1    AE    RE*100\")\n",
    "    print (AUC_Hidden[:,column_list])\n",
    "\n",
    "#    AUC_Hidden  =  np.reshape(AUC_Hidden, (-1, 10))\n",
    "#    np.savetxt(path +  \"AUC_Hidden.csv\", AUC_Hidden, delimiter=\",\", fmt='%f' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stand alone\n",
      "+ Scaler:  maxabs\n",
      "+ Corruptions:  [0.1, 0.1, 0.1]\n",
      "\n",
      "1. maiwilab ...\n",
      " + Hidden Sizes:  63 [85, 49, 12] - Batch_sizes: 100\n",
      " + Data: 68776 (55021 train, 13755 vali) - test: 17195 normal, 2806 anomaly\n",
      " + Patience:     1, Validate:     5,  \n",
      " + Batch size:   100, n batch:  550\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "norm         = \"maxabs\"                \n",
    "corruptions  = [0.1, 0.1, 0.1]\n",
    "\n",
    "print (\"stand alone\")\n",
    "#print (\"+ Data: \", list_data)\n",
    "print (\"+ Scaler: \", norm)\n",
    "print (\"+ Corruptions: \", corruptions)\n",
    "\n",
    "AUC_Hidden = np.empty([0,10])     \n",
    "\n",
    "num = 0\n",
    "data=\"maiwilab\"\n",
    "num = num + 1\n",
    "h_sizes =[85, 49, 12] #hyper_parameters(data)                   \n",
    "Xtrain=np.load('IDS_x_train.npy')\n",
    "Xtest=np.load('IDS_x_test.npy')\n",
    "total=100000\n",
    "x=np.concatenate((Xtrain,Xtest),axis=0)[:total]\n",
    "\n",
    "\n",
    "Ytrain=np.load('IDS_y_train.npy')\n",
    "Ytest=np.load('IDS_y_test.npy')\n",
    "label=np.concatenate((Ytrain,Ytest),axis=0)[:total]\n",
    "x1=pd.DataFrame(x)\n",
    "#actual=np.array([0 if i==1 else 1 for i in label])\n",
    "x1['label']=label\n",
    "d=x1.values\n",
    "d = d[~np.isnan(d).any(axis=1)]    #discard the '?' values\n",
    "seed=0\n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(d)\n",
    "\n",
    "dX = d[:,0:-1]              #put data to dX without the last column (labels)\n",
    "dy = d[:,-1].astype(int)                #put label to dy\n",
    "dy = dy > 0\n",
    "\n",
    "                            # dy=True with anomaly labels\n",
    "                            # separate into normal and anomaly\n",
    "dX0 = dX[~dy]               # Normal data \n",
    "dX1 = dX[dy]                # Anomaly data\n",
    "dy0 = dy[~dy]               # Normal label\n",
    "dy1 = dy[dy]                # Anomaly label\n",
    "\n",
    "#print(\"Normal: %d Anomaly %d\" %(len(dX0), len(dX1)))\n",
    "split = 0.8             #split 80% for training, 20% for testing\n",
    "\n",
    "idx0  = int(split * len(dX0))\n",
    "idx1  = int(split * len(dX1))\n",
    "\n",
    "train_set = dX0[:idx0]        # train_X is 80% of the normal class\n",
    "\n",
    "# test set is the other half of the normal class and all of the anomaly class\n",
    "test_set = np.concatenate((dX0[idx0:], dX1[idx1:]))  # 30% of normal and 30% of anomaly\n",
    "test_y = np.concatenate((dy0[idx0:], dy1[idx1:]))  # 30% of normal and 30% of anomaly label\n",
    "#conver test_y into 1 or 0 for computing AUC later\n",
    "actual = (~test_y).astype(np.int)\n",
    "\n",
    "train_X, test_X = normalize_data(train_set, test_set, norm)\n",
    "train_X = theano.shared(numpy.asarray(train_X, dtype=theano.config.floatX), borrow=True)\n",
    "test_X  = theano.shared(numpy.asarray(test_X,  dtype=theano.config.floatX), borrow=True)\n",
    "\n",
    "datasets = [(train_X), (test_X), (actual)]          \n",
    "\n",
    "in_dim   = train_set.shape[1]                       \n",
    "n_vali   = (int)(train_set.shape[0]/5)              \n",
    "n_train  = len(train_set) - n_vali                  \n",
    "#batch     = int(n_train/20)                          \n",
    "\n",
    "pat, val, batch, n_batch = stopping_para_shrink(n_train)\n",
    "\n",
    "\n",
    "print (\"\\n\" + str(num) + \".\", data, \"...\" )\n",
    "print (\" + Hidden Sizes: \",in_dim, h_sizes, \"- Batch_sizes:\", batch)\n",
    "print (\" + Data: %d (%d train, %d vali) - test: %d normal, %d anomaly\"\\\n",
    "%(len(train_set), n_train, n_vali, \\\n",
    "len(test_set[(actual == 1)]), len(test_set[(actual == 0)])))\n",
    "print(\" + Patience: %5.0d, Validate: %5.0d,  \\n + Batch size: %5.0d, n batch:%5.0d\"\\\n",
    "%(pat, val, batch, n_batch))\n",
    "\n",
    "\n",
    "\n",
    "#*******Computer AUC on hidden data*************\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cen\n",
      "AUC 0.5134271325289119\n",
      "Precision 0.8751677852348994\n",
      "Recall 0.9100319860424542\n",
      "f1_score 0.8922594440484676\n",
      "TPR 0.9100319860424542\n",
      "FPR 0.7954383464005702\n",
      "KDE AE/DensityBasedOneClassClassifier\n",
      "AUC 0.4813001757335929\n",
      "Precision 0.8679403061880869\n",
      "Recall 0.7847048560628089\n",
      "f1_score 0.8242265049937386\n",
      "TPR 0.7847048560628089\n",
      "FPR 0.7316464718460441\n",
      "SVM05\n",
      "AUC 0.49501993505795017\n",
      "Precision 0.8760077394388907\n",
      "Recall 0.947891829020064\n",
      "f1_score 0.9105332253289015\n",
      "TPR 0.947891829020064\n",
      "FPR 0.8221667854597291\n",
      "SVM01\n",
      "AUC 0.47866133241255754\n",
      "Precision 0.8678240420528427\n",
      "Recall 0.7296888630415819\n",
      "f1_score 0.7927842542571005\n",
      "TPR 0.7296888630415819\n",
      "FPR 0.6810406272273699\n"
     ]
    }
   ],
   "source": [
    "lof,cen,dis,kde,svm05,svm01  =auc_density(train_set,test_set, actual, norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'auc_density' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e3fcde7da9f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlof\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkde\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msvm05\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msvm01\u001b[0m  \u001b[0;34m=\u001b[0m\u001b[0mauc_density\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'auc_density' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "auc_hidden = np.column_stack([batch, re[0], lof, cen, dis, kde, svm05, svm01, ae , 100*re[2]])\n",
    "AUC_Hidden = np.append(AUC_Hidden, auc_hidden)\n",
    "\n",
    "#save hidden data to files\n",
    "#        sda.Save_Hidden_Data(train_X, test_X, data, path)\n",
    "\n",
    "#Display for saving to document\n",
    "#        AUC_RE   = np.append(AUC_RE, auc_hidden)\n",
    "#        AUC_RE   = np.reshape(AUC_RE,(-1,10))\n",
    "#\n",
    "#        np.set_printoptions(precision=3, suppress=True)\n",
    "#        column_list = [2,3,4,5,6,7,8,9]\n",
    "#        print (AUC_RE[:,column_list])\n",
    "\n",
    "AUC_Hidden  =  np.reshape(AUC_Hidden, (-1, 10))\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "column_list = [2,3,4,5,6,7,8,9]\n",
    "print(\"    LOF    CEN    MDIS   KDE   SVM5    SVM1    AE    RE*100\")\n",
    "print (AUC_Hidden[:,column_list])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((63820, 63), (12500, 63))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain.shape,Xtest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78943, 63)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:lr] *",
   "language": "python",
   "name": "conda-env-lr-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
