{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.configdefaults): install mkl with `conda install mkl-service`: No module named 'mkl'\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Aug 15 18:12:38 2017\n",
    "\n",
    "@author: VANLOI\n",
    "\"\"\"\n",
    "from __future__ import print_function, division\n",
    "\n",
    "#import sys\n",
    "import numpy\n",
    "import numpy as np\n",
    "import downhill\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "import pandas as pd\n",
    "from hiddenLayer import HiddenLayer\n",
    "from hiddenLayer import dA\n",
    "\n",
    "from ProcessingData import load_data, normalize_data\n",
    "from Methods import auc_density, auc_AEbased\n",
    "from Plot_Curves import Plotting_End2End_RE, Plotting_Pre_RE, Plotting_AUC_RE, Plotting_Pre_RE1\n",
    "from Plot_Curves import Plotting_AUC_Batch_Size, Plotting_Monitor, plot_auc_size_input, visualize_hidden1\n",
    "from Plot_Curves import Plotting_Loss_Component, plot_auc_size_1, plot_auc_size_2\n",
    "from nnet_architecture import hyper_parameters\n",
    "from stopping_para import stopping_para_shrink\n",
    "#import timeit as tm\n",
    "\n",
    "path = \"./Results/Exp_Hidden/\"\n",
    "\n",
    "def check_weight_update(sda):\n",
    "    np.set_printoptions(precision=4, suppress=True)\n",
    "    \"Check whether weights matrix is updated or not\"\n",
    "    for i in range(sda.n_layers):\n",
    "        print(\"\\n %d\" %i)\n",
    "        print (sda.Autoencoder_layers[i].W.get_value(borrow=True))\n",
    "\n",
    "    for j in range(sda.n_layers, 2*sda.n_layers):\n",
    "        print(\"\\n %d\" % j)\n",
    "        print (sda.Autoencoder_layers[j].W.eval())\n",
    "\n",
    "\n",
    "\n",
    "class SdA(object):\n",
    "    def __init__(self, numpy_rng, theano_rng=None, n_ins=100,\n",
    "                 hidden_layers_sizes=[50, 30], corruption_levels=[0.1, 0.1]):\n",
    "\n",
    "        self.encoder = []\n",
    "        self.dA_layers = []\n",
    "        self.params = []\n",
    "        self.n_layers = len(hidden_layers_sizes)\n",
    "        self.decoder = []\n",
    "\n",
    "        assert self.n_layers > 0\n",
    "        if not theano_rng:\n",
    "            theano_rng = RandomStreams(numpy_rng.randint(2 ** 30))\n",
    "\n",
    "        self.x = T.matrix('x') \n",
    "\n",
    "        \"*************** Encoder **************************\"\n",
    "        for i in range(self.n_layers):\n",
    "\n",
    "            if i == 0:\n",
    "                input_size = n_ins\n",
    "                layer_input = self.x\n",
    "            else:\n",
    "                input_size = hidden_layers_sizes[i - 1]\n",
    "                layer_input = self.encoder[-1].output\n",
    "            act_function = T.tanh\n",
    "\n",
    "            encoder_layer = HiddenLayer(rng=numpy_rng,\n",
    "                                    input       = layer_input,\n",
    "                                    n_in        = input_size,\n",
    "                                    n_out       = hidden_layers_sizes[i],\n",
    "                                    activation  = act_function)\n",
    "            self.encoder.append(encoder_layer)\n",
    "            self.params.extend(encoder_layer.params)\n",
    "\n",
    "            \"**************************************************\"\n",
    "            \"\"\"Construct a dAE that shared weights with this layer\"\"\"\n",
    "            dA_layer = dA(numpy_rng = numpy_rng,\n",
    "                          theano_rng = theano_rng,\n",
    "                          #input = layer_input,               \n",
    "                          n_visible = input_size,\n",
    "                          n_hidden  = hidden_layers_sizes[i],\n",
    "                          W         = encoder_layer.W,\n",
    "                          bhid      = encoder_layer.b)             \n",
    "\n",
    "            self.dA_layers.append(dA_layer)\n",
    "\n",
    "        \"*************** Decoder *****************************\"\n",
    "        i = self.n_layers-1\n",
    "        while (i >=0):\n",
    "            input_size = hidden_layers_sizes[i]\n",
    "            if ( i > 0):\n",
    "                output_size = hidden_layers_sizes[i-1]\n",
    "            else:\n",
    "                output_size =  n_ins\n",
    "\n",
    "            if (i==self.n_layers-1):\n",
    "                layer_input = self.encoder[-1].output\n",
    "            else:\n",
    "                layer_input = self.decoder[-1].output\n",
    "\n",
    "            decoder_layer = HiddenLayer(rng     = numpy_rng,\n",
    "                                        input   = layer_input,\n",
    "                                        n_in    = input_size,\n",
    "                                        n_out   = output_size,\n",
    "                                        activation = T.tanh,\n",
    "                                        W = self.encoder[i].W.T,\n",
    "                                        b = self.dA_layers[i].b_prime)    #this is bvis of dA\n",
    "            self.decoder.append(decoder_layer)\n",
    "            self.params.append(decoder_layer.b)\n",
    "            i = i - 1\n",
    "\n",
    "        \"******************* End To End Cost function ************************\"\n",
    "        y = self.decoder[-1].output\n",
    "        self.recon = (((self.x - y)**2).mean(1)).mean()\n",
    "        self.end2end_cost = self.recon\n",
    "\n",
    "\n",
    "    \"****** Error on train_x and valid_x before optimization process **********\"\n",
    "    def Loss_train_valid(self, train_x, valid_x):\n",
    "        index = T.lscalar('index')\n",
    "\n",
    "        train_size = train_x.get_value().shape[0]\n",
    "        tm = theano.function([index],\n",
    "                             outputs = self.end2end_cost,\n",
    "                             givens={self.x: train_x[index : train_size]})\n",
    "\n",
    "        valid_size = valid_x.get_value().shape[0]\n",
    "        vm = theano.function([index],\n",
    "                             outputs = self.end2end_cost,\n",
    "                             givens={self.x: valid_x[index : valid_size]})\n",
    "\n",
    "        return tm(0), vm(0)\n",
    "\n",
    "\n",
    "\n",
    "    \"Get data from the middle hidden layer Deep Autoencoder\"\n",
    "    def get_hidden_data(self,data_set):\n",
    "\n",
    "        data_size = data_set.get_value().shape[0]\n",
    "        index = T.lscalar('index')\n",
    "        hidden_data = theano.function([index],\n",
    "                                      outputs = self.encoder[-1].output,\n",
    "                                      givens={self.x: data_set[index : data_size]})\n",
    "        return hidden_data(0)\n",
    "\n",
    "    \"Get hidden data from hidden layer i-th for pre-training\"\n",
    "    def get_hidden_i(self,data_set, i):\n",
    "\n",
    "        data_size = data_set.get_value().shape[0]\n",
    "        index = T.lscalar('index')\n",
    "        hidden_data = theano.function([index],\n",
    "                                      outputs = self.encoder[i].output,\n",
    "                                      givens={self.x: data_set[index : data_size]})\n",
    "        return hidden_data(0)\n",
    "\n",
    "    \"Get data from the output of Autoencoder\"\n",
    "    def get_output_data(self,data_set):\n",
    "\n",
    "        data_size = data_set.get_value().shape[0]\n",
    "        index = T.lscalar('index')\n",
    "        output_data = theano.function([index],\n",
    "                                      outputs = self.decoder[-1].output,\n",
    "                                      givens={self.x: data_set[index : data_size]})\n",
    "        return output_data(0)\n",
    "\n",
    "\n",
    "#************* Pre-traing for 200 epoches, no early-stopping *************\"\n",
    "    def pretrain_Early_stopping(self, numpy_rng, train_set, n_validate, data_name,\n",
    "                                batch_size, pre_lr, corruptions):\n",
    "\n",
    "        RE = np.empty([10000, self.n_layers])\n",
    "        stop_epoch = np.empty([self.n_layers])\n",
    "\n",
    "        for i in range(self.n_layers):\n",
    "            cost, updates = self.dA_layers[i].get_cost_updates(corruptions[i], pre_lr)\n",
    "            if (i == 0):\n",
    "                train_x1 = train_set.get_value()\n",
    "            else:\n",
    "                train_x1 = self.get_hidden_i(train_set, i-1)\n",
    "\n",
    "            valid_x = train_x1[:n_validate]\n",
    "            train_x = train_x1[n_validate:]\n",
    "            # adadelta, 'adagrad (default 0.01)' 'adam''esgd' 'nag''rmsprop' 'rprop' 'sgd'\n",
    "            opt = downhill.build(algo = 'sgd', params= self.dA_layers[i].params, loss = cost)\n",
    "            train = downhill.Dataset(train_x, batch_size = batch_size, rng = numpy_rng)\n",
    "            valid = downhill.Dataset(valid_x, batch_size = len(valid_x), rng = numpy_rng)\n",
    "\n",
    "            epoch = 0\n",
    "            re = np.empty([10000])\n",
    "            for tm1, vm1 in opt.iterate(train,\n",
    "                                      valid,\n",
    "                                      patience = 100,            #100\n",
    "                                      validate_every= 5,         #5\n",
    "                                      min_improvement = 1e-3,    #4\n",
    "                                      learning_rate = pre_lr,    #1e-2\n",
    "                                      momentum = 0.0,\n",
    "                                      nesterov = False):\n",
    "                re[epoch] = tm1['loss']\n",
    "                epoch = epoch +1\n",
    "                if (epoch == 200):\n",
    "                    break\n",
    "\n",
    "            RE[:,i] = re\n",
    "            stop_epoch[i] = epoch\n",
    "\n",
    "        print (' + Stopping epoch:', stop_epoch)\n",
    "        Plotting_Pre_RE1(RE, stop_epoch, self.n_layers, 0.0, 0.1, batch_size, data_name, path)\n",
    "\n",
    "\n",
    "    \"Compute AUC for OCCs on latent data\"\n",
    "    def Compute_AUC_Hidden(self, train_set, test_set, actual, norm, data_name):\n",
    "\n",
    "        output_test  = self.get_output_data(test_set)       \n",
    "        train_hidden = self.get_hidden_data(train_set)     \n",
    "        test_hidden  = self.get_hidden_data(test_set)      \n",
    "\n",
    "        \"Compute performance of classifiers on latent data\"\n",
    "        lof, cen, dis, kde, svm05, svm01 = auc_density(train_hidden, test_hidden, actual, norm)\n",
    "        ae                               = auc_AEbased(test_set.get_value(), output_test, actual)\n",
    "        return lof, cen, dis, kde, svm05, svm01, ae\n",
    "\n",
    "    \"**************************************************************************\"\n",
    "    def Save_Hidden_Data(self, train_set, test_set, data_name, path):\n",
    "\n",
    "        train_hidden = self.get_hidden_data(train_set)      \n",
    "        test_hidden  = self.get_hidden_data(test_set)      \n",
    "        np.savetxt(path + data_name + \"_train_z.csv\", train_hidden, delimiter=\",\", fmt='%f' )\n",
    "        np.savetxt(path + data_name + \"_test_z.csv\", test_hidden, delimiter=\",\", fmt='%f' )\n",
    "\n",
    "\n",
    "    \"******** Training End-to-End Early-stopping by Downhill Package *********\"\n",
    "    def End2end_Early_stopping(self, numpy_rng, dataset, n_validate, data_name,\n",
    "                               batch_size, end2end_lr, algo, norm, patience, validation):\n",
    "\n",
    "        train_X, test_X, actual = dataset\n",
    "        valid_x = train_X.get_value()[:n_validate]\n",
    "        train_x = train_X.get_value()[n_validate:]\n",
    "        #train_x = train_x[:100]\n",
    "\n",
    "        \"for compute tm and vm before optimization process\"\n",
    "        t = theano.shared(numpy.asarray(train_x, dtype=theano.config.floatX), borrow=True)\n",
    "        v = theano.shared(numpy.asarray(valid_x, dtype=theano.config.floatX), borrow=True)\n",
    "\n",
    "        \"Use downhill for training network\"\n",
    "\n",
    "        opt = downhill.build(algo = algo, params= self.params,\n",
    "                             loss = self.end2end_cost, inputs = [self.x])\n",
    "\n",
    "        train = downhill.Dataset(train_x, batch_size = batch_size, rng = numpy_rng)\n",
    "        valid = downhill.Dataset(valid_x, batch_size = len(valid_x), rng = numpy_rng)\n",
    "\n",
    "        \"for monitoring before optimization process\"\n",
    "        stop_ep = 0\n",
    "\n",
    "\n",
    "        for tm1, vm1 in opt.iterate(train,                        \n",
    "                                  valid,\n",
    "                                  patience = patience,                \n",
    "                                  validate_every= validation,           \n",
    "                                  min_improvement = 1e-3,       \n",
    "                                  #learning_rate =  end2end_lr,\n",
    "                                  momentum = 0.0,\n",
    "                                  nesterov = False):\n",
    "\n",
    "\n",
    "            stop_ep = stop_ep + 1\n",
    "#\n",
    "##            \"******* Classification Results after End to End training ******\"\n",
    "#            if ((stop_ep%1 == 0) and (stop_ep > 0)):\n",
    "#                lof,cen,dis,kde,svm05,svm01,ae = self.Compute_AUC_Hidden(train_X, test_X, actual, norm, data_name)\n",
    "#                a = [stop_ep, lof, cen, dis, kde, svm05, svm01, ae]\n",
    "#            monitor = np.append(monitor, a)\n",
    "\n",
    "            if (stop_ep >= 1000):\n",
    "                break\n",
    "\n",
    "        #Plotting AUC and save to csv file\n",
    "#        monitor = np.reshape(monitor, (-1,8))\n",
    "#        Plotting_Monitor(monitor, 0.4, 1.0, data_name, path)\n",
    "#        np.savetxt(path + data_name + \"_monitor_auc.csv\", monitor, delimiter=\",\", fmt='%f' )\n",
    "\n",
    "        return  [stop_ep, vm1['loss'], tm1['loss']]\n",
    "\n",
    "\n",
    "\n",
    "def train_SdAE(pre_lr=0.01, end2end_lr=1e-4, algo = 'sgd',\n",
    "             dataset=[], data_name = \"WBC\", n_validate = 0, norm = \"maxabs\",\n",
    "             batch_size=10, hidden_sizes = [1,1,1], corruptions = [0.0, 0.0, 0.0],\n",
    "             patience = 1, validation = 1):\n",
    "\n",
    "    numpy_rng = numpy.random.RandomState(89677)   \n",
    "    train_X, test_X, actual = dataset             \n",
    "\n",
    "    input_size = train_X.get_value().shape[1]     \n",
    "    train_x    = train_X.get_value()[n_validate:]   \n",
    "    n_train_batches   = train_x.shape[0]\n",
    "    n_train_batches //= batch_size                  \n",
    "\n",
    "    sda = SdA(numpy_rng = numpy_rng, n_ins = input_size,\n",
    "              hidden_layers_sizes = hidden_sizes)\n",
    "\n",
    "    #PRE-TRAIN MODEL LAYER-WISE\n",
    "    sda.pretrain_Early_stopping(numpy_rng, train_X, n_validate, data_name,\n",
    "                                 batch_size, pre_lr, corruptions)\n",
    "\n",
    "    RE = sda.End2end_Early_stopping(numpy_rng, dataset, n_validate, data_name,\n",
    "                               batch_size, end2end_lr, algo, norm, patience, validation)\n",
    "\n",
    "    return sda, RE\n",
    "\n",
    "\n",
    "def Main_Test():\n",
    "\n",
    "    list_data = [\"PageBlocks\", \"WPBC\", \"PenDigits\", \"GLASS\", \"Shuttle\", \"Arrhythmia\",\\\n",
    "                 \"CTU13_10\", \"CTU13_08\",\"CTU13_09\",\"CTU13_13\",\\\n",
    "                 \"Spambase\", \"UNSW\", \"NSLKDD\", \"InternetAds\"]\n",
    "\n",
    "    list_data = [\"CTU13_10\"]\n",
    "\n",
    "    norm         = \"maxabs\"                \n",
    "    corruptions  = [0.1, 0.1, 0.1]\n",
    "\n",
    "    print (\"DAE\")\n",
    "    print (\"+ Data: \", list_data)\n",
    "    print (\"+ Scaler: \", norm)\n",
    "    print (\"+ Corruptions: \", corruptions)\n",
    "\n",
    "    AUC_Hidden = np.empty([0,10])     \n",
    "\n",
    "    num = 0\n",
    "    for data in list_data:\n",
    "        num = num + 1\n",
    "        h_sizes = hyper_parameters(data)                   \n",
    "\n",
    "        train_set, test_set, actual = load_data(data)      \n",
    "        train_X, test_X = normalize_data(train_set, test_set, norm) \n",
    "\n",
    "        train_X = theano.shared(numpy.asarray(train_X, dtype=theano.config.floatX), borrow=True)\n",
    "        test_X  = theano.shared(numpy.asarray(test_X,  dtype=theano.config.floatX), borrow=True)\n",
    "\n",
    "        datasets = [(train_X), (test_X), (actual)]          \n",
    "\n",
    "        in_dim   = train_set.shape[1]                       \n",
    "        n_vali   = (int)(train_set.shape[0]/5)              \n",
    "        n_train  = len(train_set) - n_vali                  \n",
    "        #batch     = int(n_train/20)                          \n",
    "\n",
    "        pat, val, batch, n_batch = stopping_para_shrink(n_train)\n",
    "\n",
    "\n",
    "        print (\"\\n\" + str(num) + \".\", data, \"...\" )\n",
    "        print (\" + Hidden Sizes: \",in_dim, h_sizes, \"- Batch_sizes:\", batch)\n",
    "        print (\" + Data: %d (%d train, %d vali) - %d normal, %d anomaly\"\\\n",
    "            %(len(train_set), n_train, n_vali, \\\n",
    "            len(test_set[(actual == 1)]), len(test_set[(actual == 0)])))\n",
    "        print(\" + Patience: %5.0d, Validate: %5.0d,  \\n + Batch size: %5.0d, n batch:%5.0d\"\\\n",
    "             %(pat, val, batch, n_batch))\n",
    "\n",
    "        sda, re = train_SdAE(pre_lr       = 1e-2,             \n",
    "                            end2end_lr   = 1e-4,\n",
    "                            algo         = 'adadelta',\n",
    "                            dataset      = datasets,\n",
    "                            data_name    = data,\n",
    "                            n_validate   = n_vali,\n",
    "                            norm         = norm,\n",
    "                            batch_size   = batch,\n",
    "                            hidden_sizes = h_sizes,\n",
    "                            corruptions  = corruptions,\n",
    "                            patience     = pat,\n",
    "                            validation   = val)\n",
    "\n",
    "\n",
    "        #*******Computer AUC on hidden data*************\n",
    "        lof,cen,dis,kde,svm05,svm01,ae  = sda.Compute_AUC_Hidden(train_X, test_X, actual, norm, data)\n",
    "        auc_hidden = np.column_stack([batch, re[0], lof, cen, dis, kde, svm05, svm01, ae , 100*re[2]])\n",
    "        AUC_Hidden = np.append(AUC_Hidden, auc_hidden)\n",
    "\n",
    "        #save hidden data to files\n",
    "#        sda.Save_Hidden_Data(train_X, test_X, data, path)\n",
    "\n",
    "    AUC_Hidden  =  np.reshape(AUC_Hidden, (-1, 10))\n",
    "    np.set_printoptions(precision=3, suppress=True)\n",
    "    column_list = [2,3,4,5,6,7,8,9]\n",
    "    print(\"    LOF    CEN    MDIS   KDE   SVM5    SVM1    AE    RE*100\")\n",
    "    print (AUC_Hidden[:,column_list])\n",
    "#    AUC_Hidden  =  np.reshape(AUC_Hidden, (-1, 10))\n",
    "#    np.savetxt(path +  \"AUC_Hidden.csv\", AUC_Hidden, delimiter=\",\", fmt='%f' )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DAE\n",
      "+ Data:  ['maiwilab']\n",
      "+ Scaler:  maxabs\n",
      "+ Corruptions:  [0.1, 0.1, 0.1]\n"
     ]
    }
   ],
   "source": [
    "list_data = [\"maiwilab\"]\n",
    "\n",
    "norm         = \"maxabs\"                \n",
    "corruptions  = [0.1, 0.1, 0.1]\n",
    "\n",
    "print (\"DAE\")\n",
    "print (\"+ Data: \", list_data)\n",
    "print (\"+ Scaler: \", norm)\n",
    "print (\"+ Corruptions: \", corruptions)\n",
    "\n",
    "AUC_Hidden = np.empty([0,10])     \n",
    "\n",
    "num = 0\n",
    "data=list_data[0]\n",
    "num = num + 1\n",
    "h_sizes =[85, 49, 12] #hyper_parameters(data)                   \n",
    "\n",
    "#train_set, test_set, actual = load_data(data)#normal 是1 anormal是0     \n",
    "#train_X, test_X = normalize_data(train_set, test_set, norm) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Xtrain=np.load('IDS_x_train.npy')\n",
    "Xtest=np.load('IDS_x_test.npy')\n",
    "total=100000\n",
    "x=np.concatenate((Xtrain,Xtest),axis=0)[:total]\n",
    "\n",
    "\n",
    "Ytrain=np.load('IDS_y_train.npy')\n",
    "Ytest=np.load('IDS_y_test.npy')\n",
    "label=np.concatenate((Ytrain,Ytest),axis=0)[:total]\n",
    "x1=pd.DataFrame(x)\n",
    "#actual=np.array([0 if i==1 else 1 for i in label])\n",
    "x1['label']=label\n",
    "d=x1.values\n",
    "# train_set=x1[:240000].values\n",
    "# test_set=x1[240000:300000].values\n",
    "#train_X, test_X = normalize_data(train_set, test_set, norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = d[~np.isnan(d).any(axis=1)]    #discard the '?' values\n",
    "seed=0\n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(d)\n",
    "\n",
    "dX = d[:,0:-1]              #put data to dX without the last column (labels)\n",
    "dy = d[:,-1].astype(int)                #put label to dy\n",
    "dy = dy > 0\n",
    "\n",
    "                            # dy=True with anomaly labels\n",
    "                            # separate into normal and anomaly\n",
    "dX0 = dX[~dy]               # Normal data \n",
    "dX1 = dX[dy]                # Anomaly data\n",
    "dy0 = dy[~dy]               # Normal label\n",
    "dy1 = dy[dy]                # Anomaly label\n",
    "\n",
    "#print(\"Normal: %d Anomaly %d\" %(len(dX0), len(dX1)))\n",
    "split = 0.8             #split 80% for training, 20% for testing\n",
    "\n",
    "idx0  = int(split * len(dX0))\n",
    "idx1  = int(split * len(dX1))\n",
    "\n",
    "train_set = dX0[:idx0]        # train_X is 80% of the normal class\n",
    "\n",
    "# test set is the other half of the normal class and all of the anomaly class\n",
    "test_set = np.concatenate((dX0[idx0:], dX1[idx1:]))  # 30% of normal and 30% of anomaly\n",
    "test_y = np.concatenate((dy0[idx0:], dy1[idx1:]))  # 30% of normal and 30% of anomaly label\n",
    "#conver test_y into 1 or 0 for computing AUC later\n",
    "actual = (~test_y).astype(np.int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(68776, 63)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, ...,  True,  True,  True])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20001,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. maiwilab ...\n",
      " + Hidden Sizes:  63 [85, 49, 12] - Batch_sizes: 100\n",
      " + Data: 68776 (55021 train, 13755 vali) - test: 17195 normal, 2806 anomaly\n",
      " + Patience:     1, Validate:     5,  \n",
      " + Batch size:   100, n batch:  550\n"
     ]
    }
   ],
   "source": [
    "train_X, test_X = normalize_data(train_set, test_set, norm)\n",
    "train_X = theano.shared(numpy.asarray(train_X, dtype=theano.config.floatX), borrow=True)\n",
    "test_X  = theano.shared(numpy.asarray(test_X,  dtype=theano.config.floatX), borrow=True)\n",
    "\n",
    "datasets = [(train_X), (test_X), (actual)]          \n",
    "\n",
    "in_dim   = train_set.shape[1]                       \n",
    "n_vali   = (int)(train_set.shape[0]/5)              \n",
    "n_train  = len(train_set) - n_vali                  \n",
    "#batch     = int(n_train/20)                          \n",
    "\n",
    "pat, val, batch, n_batch = stopping_para_shrink(n_train)\n",
    "\n",
    "\n",
    "print (\"\\n\" + str(num) + \".\", data, \"...\" )\n",
    "print (\" + Hidden Sizes: \",in_dim, h_sizes, \"- Batch_sizes:\", batch)\n",
    "print (\" + Data: %d (%d train, %d vali) - test: %d normal, %d anomaly\"\\\n",
    "    %(len(train_set), n_train, n_vali, \\\n",
    "    len(test_set[(actual == 1)]), len(test_set[(actual == 0)])))\n",
    "print(\" + Patience: %5.0d, Validate: %5.0d,  \\n + Batch size: %5.0d, n batch:%5.0d\"\\\n",
    "     %(pat, val, batch, n_batch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset0: 551 of 551 mini-batches from (55021, 63)\n",
      "dataset1: 1 of 1 mini-batches from (13755, 63)\n",
      "\u001b[36mdownhill\u001b[0m: compiling evaluation function\n",
      "\u001b[36mdownhill\u001b[0m: compiling \u001b[31mSGD\u001b[0m optimizer\n",
      "\u001b[36mdownhill\u001b[0m: setting: patience\u001b[0m = \u001b[33m100\u001b[0m\n",
      "\u001b[36mdownhill\u001b[0m: setting: validate_every\u001b[0m = \u001b[33m5\u001b[0m\n",
      "\u001b[36mdownhill\u001b[0m: setting: min_improvement\u001b[0m = \u001b[33m0.001\u001b[0m\n",
      "\u001b[36mdownhill\u001b[0m: setting: max_gradient_norm\u001b[0m = \u001b[33m0\u001b[0m\n",
      "\u001b[36mdownhill\u001b[0m: setting: max_gradient_elem\u001b[0m = \u001b[33m0\u001b[0m\n",
      "\u001b[36mdownhill\u001b[0m: setting: learning_rate\u001b[0m = \u001b[33mTensorConstant{0.01}\u001b[0m\n",
      "\u001b[36mdownhill\u001b[0m: setting: momentum\u001b[0m = \u001b[33m0.0\u001b[0m\n",
      "\u001b[36mdownhill\u001b[0m: setting: nesterov\u001b[0m = \u001b[33mFalse\u001b[0m\n",
      "\u001b[36mdownhill\u001b[0m: validation 0 loss=0.106617 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 1 loss=0.069125\n",
      "\u001b[36mdownhill\u001b[0m: SGD 2 loss=0.043164\n",
      "\u001b[36mdownhill\u001b[0m: SGD 3 loss=0.036223\n",
      "\u001b[36mdownhill\u001b[0m: SGD 4 loss=0.032121\n",
      "\u001b[36mdownhill\u001b[0m: SGD 5 loss=0.029195\n",
      "\u001b[36mdownhill\u001b[0m: validation 1 loss=0.027893 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 6 loss=0.026690\n",
      "\u001b[36mdownhill\u001b[0m: SGD 7 loss=0.024716\n",
      "\u001b[36mdownhill\u001b[0m: SGD 8 loss=0.023099\n",
      "\u001b[36mdownhill\u001b[0m: SGD 9 loss=0.021704\n",
      "\u001b[36mdownhill\u001b[0m: SGD 10 loss=0.020581\n",
      "\u001b[36mdownhill\u001b[0m: validation 2 loss=0.020009 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 11 loss=0.019589\n",
      "\u001b[36mdownhill\u001b[0m: SGD 12 loss=0.018796\n",
      "\u001b[36mdownhill\u001b[0m: SGD 13 loss=0.017964\n",
      "\u001b[36mdownhill\u001b[0m: SGD 14 loss=0.017323\n",
      "\u001b[36mdownhill\u001b[0m: SGD 15 loss=0.016770\n",
      "\u001b[36mdownhill\u001b[0m: validation 3 loss=0.016435 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 16 loss=0.016217\n",
      "\u001b[36mdownhill\u001b[0m: SGD 17 loss=0.015710\n",
      "\u001b[36mdownhill\u001b[0m: SGD 18 loss=0.015307\n",
      "\u001b[36mdownhill\u001b[0m: SGD 19 loss=0.014883\n",
      "\u001b[36mdownhill\u001b[0m: SGD 20 loss=0.014493\n",
      "\u001b[36mdownhill\u001b[0m: validation 4 loss=0.014294 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 21 loss=0.014170\n",
      "\u001b[36mdownhill\u001b[0m: SGD 22 loss=0.013838\n",
      "\u001b[36mdownhill\u001b[0m: SGD 23 loss=0.013493\n",
      "\u001b[36mdownhill\u001b[0m: SGD 24 loss=0.013314\n",
      "\u001b[36mdownhill\u001b[0m: SGD 25 loss=0.013015\n",
      "\u001b[36mdownhill\u001b[0m: validation 5 loss=0.012811 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 26 loss=0.012773\n",
      "\u001b[36mdownhill\u001b[0m: SGD 27 loss=0.012487\n",
      "\u001b[36mdownhill\u001b[0m: SGD 28 loss=0.012283\n",
      "\u001b[36mdownhill\u001b[0m: SGD 29 loss=0.012032\n",
      "\u001b[36mdownhill\u001b[0m: SGD 30 loss=0.011907\n",
      "\u001b[36mdownhill\u001b[0m: validation 6 loss=0.011750 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 31 loss=0.011624\n",
      "\u001b[36mdownhill\u001b[0m: SGD 32 loss=0.011438\n",
      "\u001b[36mdownhill\u001b[0m: SGD 33 loss=0.011272\n",
      "\u001b[36mdownhill\u001b[0m: SGD 34 loss=0.011081\n",
      "\u001b[36mdownhill\u001b[0m: SGD 35 loss=0.010966\n",
      "\u001b[36mdownhill\u001b[0m: validation 7 loss=0.010837 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 36 loss=0.010789\n",
      "\u001b[36mdownhill\u001b[0m: SGD 37 loss=0.010651\n",
      "\u001b[36mdownhill\u001b[0m: SGD 38 loss=0.010495\n",
      "\u001b[36mdownhill\u001b[0m: SGD 39 loss=0.010385\n",
      "\u001b[36mdownhill\u001b[0m: SGD 40 loss=0.010248\n",
      "\u001b[36mdownhill\u001b[0m: validation 8 loss=0.010114 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 41 loss=0.010119\n",
      "\u001b[36mdownhill\u001b[0m: SGD 42 loss=0.009949\n",
      "\u001b[36mdownhill\u001b[0m: SGD 43 loss=0.009907\n",
      "\u001b[36mdownhill\u001b[0m: SGD 44 loss=0.009750\n",
      "\u001b[36mdownhill\u001b[0m: SGD 45 loss=0.009627\n",
      "\u001b[36mdownhill\u001b[0m: validation 9 loss=0.009558 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 46 loss=0.009595\n",
      "\u001b[36mdownhill\u001b[0m: SGD 47 loss=0.009458\n",
      "\u001b[36mdownhill\u001b[0m: SGD 48 loss=0.009341\n",
      "\u001b[36mdownhill\u001b[0m: SGD 49 loss=0.009263\n",
      "\u001b[36mdownhill\u001b[0m: SGD 50 loss=0.009180\n",
      "\u001b[36mdownhill\u001b[0m: validation 10 loss=0.009120 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 51 loss=0.009069\n",
      "\u001b[36mdownhill\u001b[0m: SGD 52 loss=0.008981\n",
      "\u001b[36mdownhill\u001b[0m: SGD 53 loss=0.008955\n",
      "\u001b[36mdownhill\u001b[0m: SGD 54 loss=0.008828\n",
      "\u001b[36mdownhill\u001b[0m: SGD 55 loss=0.008773\n",
      "\u001b[36mdownhill\u001b[0m: validation 11 loss=0.008768 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 56 loss=0.008689\n",
      "\u001b[36mdownhill\u001b[0m: SGD 57 loss=0.008652\n",
      "\u001b[36mdownhill\u001b[0m: SGD 58 loss=0.008556\n",
      "\u001b[36mdownhill\u001b[0m: SGD 59 loss=0.008508\n",
      "\u001b[36mdownhill\u001b[0m: SGD 60 loss=0.008464\n",
      "\u001b[36mdownhill\u001b[0m: validation 12 loss=0.008397 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 61 loss=0.008365\n",
      "\u001b[36mdownhill\u001b[0m: SGD 62 loss=0.008287\n",
      "\u001b[36mdownhill\u001b[0m: SGD 63 loss=0.008215\n",
      "\u001b[36mdownhill\u001b[0m: SGD 64 loss=0.008188\n",
      "\u001b[36mdownhill\u001b[0m: SGD 65 loss=0.008129\n",
      "\u001b[36mdownhill\u001b[0m: validation 13 loss=0.008087 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 66 loss=0.008097\n",
      "\u001b[36mdownhill\u001b[0m: SGD 67 loss=0.008056\n",
      "\u001b[36mdownhill\u001b[0m: SGD 68 loss=0.008003\n",
      "\u001b[36mdownhill\u001b[0m: SGD 69 loss=0.007938\n",
      "\u001b[36mdownhill\u001b[0m: SGD 70 loss=0.007894\n",
      "\u001b[36mdownhill\u001b[0m: validation 14 loss=0.007958 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 71 loss=0.007863\n",
      "\u001b[36mdownhill\u001b[0m: SGD 72 loss=0.007823\n",
      "\u001b[36mdownhill\u001b[0m: SGD 73 loss=0.007756\n",
      "\u001b[36mdownhill\u001b[0m: SGD 74 loss=0.007722\n",
      "\u001b[36mdownhill\u001b[0m: SGD 75 loss=0.007688\n",
      "\u001b[36mdownhill\u001b[0m: validation 15 loss=0.007726 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 76 loss=0.007655\n",
      "\u001b[36mdownhill\u001b[0m: SGD 77 loss=0.007609\n",
      "\u001b[36mdownhill\u001b[0m: SGD 78 loss=0.007538\n",
      "\u001b[36mdownhill\u001b[0m: SGD 79 loss=0.007545\n",
      "\u001b[36mdownhill\u001b[0m: SGD 80 loss=0.007473\n",
      "\u001b[36mdownhill\u001b[0m: validation 16 loss=0.007465 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 81 loss=0.007432\n",
      "\u001b[36mdownhill\u001b[0m: SGD 82 loss=0.007426\n",
      "\u001b[36mdownhill\u001b[0m: SGD 83 loss=0.007346\n",
      "\u001b[36mdownhill\u001b[0m: SGD 84 loss=0.007343\n",
      "\u001b[36mdownhill\u001b[0m: SGD 85 loss=0.007298\n",
      "\u001b[36mdownhill\u001b[0m: validation 17 loss=0.007264 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 86 loss=0.007282\n",
      "\u001b[36mdownhill\u001b[0m: SGD 87 loss=0.007267\n",
      "\u001b[36mdownhill\u001b[0m: SGD 88 loss=0.007196\n",
      "\u001b[36mdownhill\u001b[0m: SGD 89 loss=0.007203\n",
      "\u001b[36mdownhill\u001b[0m: SGD 90 loss=0.007150\n",
      "\u001b[36mdownhill\u001b[0m: validation 18 loss=0.007164 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 91 loss=0.007148\n",
      "\u001b[36mdownhill\u001b[0m: SGD 92 loss=0.007108\n",
      "\u001b[36mdownhill\u001b[0m: SGD 93 loss=0.007108\n",
      "\u001b[36mdownhill\u001b[0m: SGD 94 loss=0.007072\n",
      "\u001b[36mdownhill\u001b[0m: SGD 95 loss=0.007023\n",
      "\u001b[36mdownhill\u001b[0m: validation 19 loss=0.007008 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 96 loss=0.007001\n",
      "\u001b[36mdownhill\u001b[0m: SGD 97 loss=0.006984\n",
      "\u001b[36mdownhill\u001b[0m: SGD 98 loss=0.006971\n",
      "\u001b[36mdownhill\u001b[0m: SGD 99 loss=0.006934\n",
      "\u001b[36mdownhill\u001b[0m: SGD 100 loss=0.006917\n",
      "\u001b[36mdownhill\u001b[0m: validation 20 loss=0.006878 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 101 loss=0.006915\n",
      "\u001b[36mdownhill\u001b[0m: SGD 102 loss=0.006877\n",
      "\u001b[36mdownhill\u001b[0m: SGD 103 loss=0.006841\n",
      "\u001b[36mdownhill\u001b[0m: SGD 104 loss=0.006827\n",
      "\u001b[36mdownhill\u001b[0m: SGD 105 loss=0.006812\n",
      "\u001b[36mdownhill\u001b[0m: validation 21 loss=0.006761 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 106 loss=0.006770\n",
      "\u001b[36mdownhill\u001b[0m: SGD 107 loss=0.006755\n",
      "\u001b[36mdownhill\u001b[0m: SGD 108 loss=0.006743\n",
      "\u001b[36mdownhill\u001b[0m: SGD 109 loss=0.006703\n",
      "\u001b[36mdownhill\u001b[0m: SGD 110 loss=0.006698\n",
      "\u001b[36mdownhill\u001b[0m: validation 22 loss=0.006715 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 111 loss=0.006694\n",
      "\u001b[36mdownhill\u001b[0m: SGD 112 loss=0.006693\n",
      "\u001b[36mdownhill\u001b[0m: SGD 113 loss=0.006641\n",
      "\u001b[36mdownhill\u001b[0m: SGD 114 loss=0.006612\n",
      "\u001b[36mdownhill\u001b[0m: SGD 115 loss=0.006579\n",
      "\u001b[36mdownhill\u001b[0m: validation 23 loss=0.006595 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 116 loss=0.006606\n",
      "\u001b[36mdownhill\u001b[0m: SGD 117 loss=0.006580\n",
      "\u001b[36mdownhill\u001b[0m: SGD 118 loss=0.006555\n",
      "\u001b[36mdownhill\u001b[0m: SGD 119 loss=0.006545\n",
      "\u001b[36mdownhill\u001b[0m: SGD 120 loss=0.006529\n",
      "\u001b[36mdownhill\u001b[0m: validation 24 loss=0.006515 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 121 loss=0.006547\n",
      "\u001b[36mdownhill\u001b[0m: SGD 122 loss=0.006492\n",
      "\u001b[36mdownhill\u001b[0m: SGD 123 loss=0.006498\n",
      "\u001b[36mdownhill\u001b[0m: SGD 124 loss=0.006458\n",
      "\u001b[36mdownhill\u001b[0m: SGD 125 loss=0.006438\n",
      "\u001b[36mdownhill\u001b[0m: validation 25 loss=0.006475 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 126 loss=0.006447\n",
      "\u001b[36mdownhill\u001b[0m: SGD 127 loss=0.006443\n",
      "\u001b[36mdownhill\u001b[0m: SGD 128 loss=0.006410\n",
      "\u001b[36mdownhill\u001b[0m: SGD 129 loss=0.006404\n",
      "\u001b[36mdownhill\u001b[0m: SGD 130 loss=0.006403\n",
      "\u001b[36mdownhill\u001b[0m: validation 26 loss=0.006429 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 131 loss=0.006370\n",
      "\u001b[36mdownhill\u001b[0m: SGD 132 loss=0.006377\n",
      "\u001b[36mdownhill\u001b[0m: SGD 133 loss=0.006369\n",
      "\u001b[36mdownhill\u001b[0m: SGD 134 loss=0.006351\n",
      "\u001b[36mdownhill\u001b[0m: SGD 135 loss=0.006316\n",
      "\u001b[36mdownhill\u001b[0m: validation 27 loss=0.006285 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 136 loss=0.006287\n",
      "\u001b[36mdownhill\u001b[0m: SGD 137 loss=0.006293\n",
      "\u001b[36mdownhill\u001b[0m: SGD 138 loss=0.006277\n",
      "\u001b[36mdownhill\u001b[0m: SGD 139 loss=0.006267\n",
      "\u001b[36mdownhill\u001b[0m: SGD 140 loss=0.006265\n",
      "\u001b[36mdownhill\u001b[0m: validation 28 loss=0.006273 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 141 loss=0.006234\n",
      "\u001b[36mdownhill\u001b[0m: SGD 142 loss=0.006259\n",
      "\u001b[36mdownhill\u001b[0m: SGD 143 loss=0.006207\n",
      "\u001b[36mdownhill\u001b[0m: SGD 144 loss=0.006218\n",
      "\u001b[36mdownhill\u001b[0m: SGD 145 loss=0.006188\n",
      "\u001b[36mdownhill\u001b[0m: validation 29 loss=0.006194 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 146 loss=0.006177\n",
      "\u001b[36mdownhill\u001b[0m: SGD 147 loss=0.006186\n",
      "\u001b[36mdownhill\u001b[0m: SGD 148 loss=0.006159\n",
      "\u001b[36mdownhill\u001b[0m: SGD 149 loss=0.006162\n",
      "\u001b[36mdownhill\u001b[0m: SGD 150 loss=0.006178\n",
      "\u001b[36mdownhill\u001b[0m: validation 30 loss=0.006108 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 151 loss=0.006124\n",
      "\u001b[36mdownhill\u001b[0m: SGD 152 loss=0.006140\n",
      "\u001b[36mdownhill\u001b[0m: SGD 153 loss=0.006150\n",
      "\u001b[36mdownhill\u001b[0m: SGD 154 loss=0.006116\n",
      "\u001b[36mdownhill\u001b[0m: SGD 155 loss=0.006092\n",
      "\u001b[36mdownhill\u001b[0m: validation 31 loss=0.006130\n",
      "\u001b[36mdownhill\u001b[0m: SGD 156 loss=0.006092\n",
      "\u001b[36mdownhill\u001b[0m: SGD 157 loss=0.006067\n",
      "\u001b[36mdownhill\u001b[0m: SGD 158 loss=0.006078\n",
      "\u001b[36mdownhill\u001b[0m: SGD 159 loss=0.006061\n",
      "\u001b[36mdownhill\u001b[0m: SGD 160 loss=0.006037\n",
      "\u001b[36mdownhill\u001b[0m: validation 32 loss=0.006088 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 161 loss=0.006059\n",
      "\u001b[36mdownhill\u001b[0m: SGD 162 loss=0.006039\n",
      "\u001b[36mdownhill\u001b[0m: SGD 163 loss=0.006025\n",
      "\u001b[36mdownhill\u001b[0m: SGD 164 loss=0.006019\n",
      "\u001b[36mdownhill\u001b[0m: SGD 165 loss=0.006002\n",
      "\u001b[36mdownhill\u001b[0m: validation 33 loss=0.005982 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 166 loss=0.006003\n",
      "\u001b[36mdownhill\u001b[0m: SGD 167 loss=0.005990\n",
      "\u001b[36mdownhill\u001b[0m: SGD 168 loss=0.005987\n",
      "\u001b[36mdownhill\u001b[0m: SGD 169 loss=0.005962\n",
      "\u001b[36mdownhill\u001b[0m: SGD 170 loss=0.005963\n",
      "\u001b[36mdownhill\u001b[0m: validation 34 loss=0.006022\n",
      "\u001b[36mdownhill\u001b[0m: SGD 171 loss=0.005952\n",
      "\u001b[36mdownhill\u001b[0m: SGD 172 loss=0.005957\n",
      "\u001b[36mdownhill\u001b[0m: SGD 173 loss=0.005958\n",
      "\u001b[36mdownhill\u001b[0m: SGD 174 loss=0.005919\n",
      "\u001b[36mdownhill\u001b[0m: SGD 175 loss=0.005914\n",
      "\u001b[36mdownhill\u001b[0m: validation 35 loss=0.005929 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 176 loss=0.005896\n",
      "\u001b[36mdownhill\u001b[0m: SGD 177 loss=0.005928\n",
      "\u001b[36mdownhill\u001b[0m: SGD 178 loss=0.005910\n",
      "\u001b[36mdownhill\u001b[0m: SGD 179 loss=0.005899\n",
      "\u001b[36mdownhill\u001b[0m: SGD 180 loss=0.005892\n",
      "\u001b[36mdownhill\u001b[0m: validation 36 loss=0.005951\n",
      "\u001b[36mdownhill\u001b[0m: SGD 181 loss=0.005868\n",
      "\u001b[36mdownhill\u001b[0m: SGD 182 loss=0.005883\n",
      "\u001b[36mdownhill\u001b[0m: SGD 183 loss=0.005885\n",
      "\u001b[36mdownhill\u001b[0m: SGD 184 loss=0.005843\n",
      "\u001b[36mdownhill\u001b[0m: SGD 185 loss=0.005850\n",
      "\u001b[36mdownhill\u001b[0m: validation 37 loss=0.005846 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 186 loss=0.005837\n",
      "\u001b[36mdownhill\u001b[0m: SGD 187 loss=0.005842\n",
      "\u001b[36mdownhill\u001b[0m: SGD 188 loss=0.005803\n",
      "\u001b[36mdownhill\u001b[0m: SGD 189 loss=0.005853\n",
      "\u001b[36mdownhill\u001b[0m: SGD 190 loss=0.005815\n",
      "\u001b[36mdownhill\u001b[0m: validation 38 loss=0.005832 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 191 loss=0.005840\n",
      "\u001b[36mdownhill\u001b[0m: SGD 192 loss=0.005793\n",
      "\u001b[36mdownhill\u001b[0m: SGD 193 loss=0.005799\n",
      "\u001b[36mdownhill\u001b[0m: SGD 194 loss=0.005801\n",
      "\u001b[36mdownhill\u001b[0m: SGD 195 loss=0.005804\n",
      "\u001b[36mdownhill\u001b[0m: validation 39 loss=0.005803 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 196 loss=0.005785\n",
      "\u001b[36mdownhill\u001b[0m: SGD 197 loss=0.005789\n",
      "\u001b[36mdownhill\u001b[0m: SGD 198 loss=0.005771\n",
      "\u001b[36mdownhill\u001b[0m: SGD 199 loss=0.005761\n",
      "\u001b[36mdownhill\u001b[0m: SGD 200 loss=0.005757\n",
      "dataset2: 551 of 551 mini-batches from (55021, 85)\n",
      "dataset3: 1 of 1 mini-batches from (13755, 85)\n",
      "\u001b[36mdownhill\u001b[0m: compiling evaluation function\n",
      "\u001b[36mdownhill\u001b[0m: compiling \u001b[31mSGD\u001b[0m optimizer\n",
      "\u001b[36mdownhill\u001b[0m: setting: patience\u001b[0m = \u001b[33m100\u001b[0m\n",
      "\u001b[36mdownhill\u001b[0m: setting: validate_every\u001b[0m = \u001b[33m5\u001b[0m\n",
      "\u001b[36mdownhill\u001b[0m: setting: min_improvement\u001b[0m = \u001b[33m0.001\u001b[0m\n",
      "\u001b[36mdownhill\u001b[0m: setting: max_gradient_norm\u001b[0m = \u001b[33m0\u001b[0m\n",
      "\u001b[36mdownhill\u001b[0m: setting: max_gradient_elem\u001b[0m = \u001b[33m0\u001b[0m\n",
      "\u001b[36mdownhill\u001b[0m: setting: learning_rate\u001b[0m = \u001b[33mTensorConstant{0.01}\u001b[0m\n",
      "\u001b[36mdownhill\u001b[0m: setting: momentum\u001b[0m = \u001b[33m0.0\u001b[0m\n",
      "\u001b[36mdownhill\u001b[0m: setting: nesterov\u001b[0m = \u001b[33mFalse\u001b[0m\n",
      "\u001b[36mdownhill\u001b[0m: validation 0 loss=0.144446 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 1 loss=0.085517\n",
      "\u001b[36mdownhill\u001b[0m: SGD 2 loss=0.043105\n",
      "\u001b[36mdownhill\u001b[0m: SGD 3 loss=0.036317\n",
      "\u001b[36mdownhill\u001b[0m: SGD 4 loss=0.033475\n",
      "\u001b[36mdownhill\u001b[0m: SGD 5 loss=0.031236\n",
      "\u001b[36mdownhill\u001b[0m: validation 1 loss=0.030360 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 6 loss=0.029283\n",
      "\u001b[36mdownhill\u001b[0m: SGD 7 loss=0.027570\n",
      "\u001b[36mdownhill\u001b[0m: SGD 8 loss=0.026012\n",
      "\u001b[36mdownhill\u001b[0m: SGD 9 loss=0.024682\n",
      "\u001b[36mdownhill\u001b[0m: SGD 10 loss=0.023509\n",
      "\u001b[36mdownhill\u001b[0m: validation 2 loss=0.022947 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 11 loss=0.022446\n",
      "\u001b[36mdownhill\u001b[0m: SGD 12 loss=0.021519\n",
      "\u001b[36mdownhill\u001b[0m: SGD 13 loss=0.020692\n",
      "\u001b[36mdownhill\u001b[0m: SGD 14 loss=0.019902\n",
      "\u001b[36mdownhill\u001b[0m: SGD 15 loss=0.019194\n",
      "\u001b[36mdownhill\u001b[0m: validation 3 loss=0.018844 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 16 loss=0.018596\n",
      "\u001b[36mdownhill\u001b[0m: SGD 17 loss=0.018062\n",
      "\u001b[36mdownhill\u001b[0m: SGD 18 loss=0.017507\n",
      "\u001b[36mdownhill\u001b[0m: SGD 19 loss=0.017036\n",
      "\u001b[36mdownhill\u001b[0m: SGD 20 loss=0.016585\n",
      "\u001b[36mdownhill\u001b[0m: validation 4 loss=0.016345 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 21 loss=0.016171\n",
      "\u001b[36mdownhill\u001b[0m: SGD 22 loss=0.015790\n",
      "\u001b[36mdownhill\u001b[0m: SGD 23 loss=0.015431\n",
      "\u001b[36mdownhill\u001b[0m: SGD 24 loss=0.015105\n",
      "\u001b[36mdownhill\u001b[0m: SGD 25 loss=0.014802\n",
      "\u001b[36mdownhill\u001b[0m: validation 5 loss=0.014618 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 26 loss=0.014501\n",
      "\u001b[36mdownhill\u001b[0m: SGD 27 loss=0.014247\n",
      "\u001b[36mdownhill\u001b[0m: SGD 28 loss=0.013965\n",
      "\u001b[36mdownhill\u001b[0m: SGD 29 loss=0.013723\n",
      "\u001b[36mdownhill\u001b[0m: SGD 30 loss=0.013516\n",
      "\u001b[36mdownhill\u001b[0m: validation 6 loss=0.013348 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 31 loss=0.013262\n",
      "\u001b[36mdownhill\u001b[0m: SGD 32 loss=0.013044\n",
      "\u001b[36mdownhill\u001b[0m: SGD 33 loss=0.012839\n",
      "\u001b[36mdownhill\u001b[0m: SGD 34 loss=0.012674\n",
      "\u001b[36mdownhill\u001b[0m: SGD 35 loss=0.012474\n",
      "\u001b[36mdownhill\u001b[0m: validation 7 loss=0.012360 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 36 loss=0.012316\n",
      "\u001b[36mdownhill\u001b[0m: SGD 37 loss=0.012127\n",
      "\u001b[36mdownhill\u001b[0m: SGD 38 loss=0.011959\n",
      "\u001b[36mdownhill\u001b[0m: SGD 39 loss=0.011825\n",
      "\u001b[36mdownhill\u001b[0m: SGD 40 loss=0.011650\n",
      "\u001b[36mdownhill\u001b[0m: validation 8 loss=0.011542 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 41 loss=0.011517\n",
      "\u001b[36mdownhill\u001b[0m: SGD 42 loss=0.011366\n",
      "\u001b[36mdownhill\u001b[0m: SGD 43 loss=0.011224\n",
      "\u001b[36mdownhill\u001b[0m: SGD 44 loss=0.011090\n",
      "\u001b[36mdownhill\u001b[0m: SGD 45 loss=0.010964\n",
      "\u001b[36mdownhill\u001b[0m: validation 9 loss=0.010854 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 46 loss=0.010838\n",
      "\u001b[36mdownhill\u001b[0m: SGD 47 loss=0.010732\n",
      "\u001b[36mdownhill\u001b[0m: SGD 48 loss=0.010608\n",
      "\u001b[36mdownhill\u001b[0m: SGD 49 loss=0.010477\n",
      "\u001b[36mdownhill\u001b[0m: SGD 50 loss=0.010389\n",
      "\u001b[36mdownhill\u001b[0m: validation 10 loss=0.010268 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 51 loss=0.010277\n",
      "\u001b[36mdownhill\u001b[0m: SGD 52 loss=0.010186\n",
      "\u001b[36mdownhill\u001b[0m: SGD 53 loss=0.010083\n",
      "\u001b[36mdownhill\u001b[0m: SGD 54 loss=0.009967\n",
      "\u001b[36mdownhill\u001b[0m: SGD 55 loss=0.009887\n",
      "\u001b[36mdownhill\u001b[0m: validation 11 loss=0.009799 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 56 loss=0.009797\n",
      "\u001b[36mdownhill\u001b[0m: SGD 57 loss=0.009703\n",
      "\u001b[36mdownhill\u001b[0m: SGD 58 loss=0.009621\n",
      "\u001b[36mdownhill\u001b[0m: SGD 59 loss=0.009537\n",
      "\u001b[36mdownhill\u001b[0m: SGD 60 loss=0.009439\n",
      "\u001b[36mdownhill\u001b[0m: validation 12 loss=0.009360 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 61 loss=0.009372\n",
      "\u001b[36mdownhill\u001b[0m: SGD 62 loss=0.009265\n",
      "\u001b[36mdownhill\u001b[0m: SGD 63 loss=0.009191\n",
      "\u001b[36mdownhill\u001b[0m: SGD 64 loss=0.009122\n",
      "\u001b[36mdownhill\u001b[0m: SGD 65 loss=0.009043\n",
      "\u001b[36mdownhill\u001b[0m: validation 13 loss=0.008964 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 66 loss=0.008980\n",
      "\u001b[36mdownhill\u001b[0m: SGD 67 loss=0.008886\n",
      "\u001b[36mdownhill\u001b[0m: SGD 68 loss=0.008845\n",
      "\u001b[36mdownhill\u001b[0m: SGD 69 loss=0.008762\n",
      "\u001b[36mdownhill\u001b[0m: SGD 70 loss=0.008701\n",
      "\u001b[36mdownhill\u001b[0m: validation 14 loss=0.008599 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 71 loss=0.008611\n",
      "\u001b[36mdownhill\u001b[0m: SGD 72 loss=0.008591\n",
      "\u001b[36mdownhill\u001b[0m: SGD 73 loss=0.008508\n",
      "\u001b[36mdownhill\u001b[0m: SGD 74 loss=0.008443\n",
      "\u001b[36mdownhill\u001b[0m: SGD 75 loss=0.008399\n",
      "\u001b[36mdownhill\u001b[0m: validation 15 loss=0.008316 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 76 loss=0.008343\n",
      "\u001b[36mdownhill\u001b[0m: SGD 77 loss=0.008261\n",
      "\u001b[36mdownhill\u001b[0m: SGD 78 loss=0.008218\n",
      "\u001b[36mdownhill\u001b[0m: SGD 79 loss=0.008141\n",
      "\u001b[36mdownhill\u001b[0m: SGD 80 loss=0.008104\n",
      "\u001b[36mdownhill\u001b[0m: validation 16 loss=0.008050 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 81 loss=0.008048\n",
      "\u001b[36mdownhill\u001b[0m: SGD 82 loss=0.007999\n",
      "\u001b[36mdownhill\u001b[0m: SGD 83 loss=0.007947\n",
      "\u001b[36mdownhill\u001b[0m: SGD 84 loss=0.007902\n",
      "\u001b[36mdownhill\u001b[0m: SGD 85 loss=0.007848\n",
      "\u001b[36mdownhill\u001b[0m: validation 17 loss=0.007771 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 86 loss=0.007800\n",
      "\u001b[36mdownhill\u001b[0m: SGD 87 loss=0.007760\n",
      "\u001b[36mdownhill\u001b[0m: SGD 88 loss=0.007689\n",
      "\u001b[36mdownhill\u001b[0m: SGD 89 loss=0.007678\n",
      "\u001b[36mdownhill\u001b[0m: SGD 90 loss=0.007607\n",
      "\u001b[36mdownhill\u001b[0m: validation 18 loss=0.007564 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 91 loss=0.007574\n",
      "\u001b[36mdownhill\u001b[0m: SGD 92 loss=0.007525\n",
      "\u001b[36mdownhill\u001b[0m: SGD 93 loss=0.007480\n",
      "\u001b[36mdownhill\u001b[0m: SGD 94 loss=0.007455\n",
      "\u001b[36mdownhill\u001b[0m: SGD 95 loss=0.007411\n",
      "\u001b[36mdownhill\u001b[0m: validation 19 loss=0.007382 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 96 loss=0.007355\n",
      "\u001b[36mdownhill\u001b[0m: SGD 97 loss=0.007315\n",
      "\u001b[36mdownhill\u001b[0m: SGD 98 loss=0.007272\n",
      "\u001b[36mdownhill\u001b[0m: SGD 99 loss=0.007241\n",
      "\u001b[36mdownhill\u001b[0m: SGD 100 loss=0.007194\n",
      "\u001b[36mdownhill\u001b[0m: validation 20 loss=0.007136 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 101 loss=0.007158\n",
      "\u001b[36mdownhill\u001b[0m: SGD 102 loss=0.007128\n",
      "\u001b[36mdownhill\u001b[0m: SGD 103 loss=0.007094\n",
      "\u001b[36mdownhill\u001b[0m: SGD 104 loss=0.007060\n",
      "\u001b[36mdownhill\u001b[0m: SGD 105 loss=0.007017\n",
      "\u001b[36mdownhill\u001b[0m: validation 21 loss=0.006988 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 106 loss=0.006989\n",
      "\u001b[36mdownhill\u001b[0m: SGD 107 loss=0.006970\n",
      "\u001b[36mdownhill\u001b[0m: SGD 108 loss=0.006931\n",
      "\u001b[36mdownhill\u001b[0m: SGD 109 loss=0.006882\n",
      "\u001b[36mdownhill\u001b[0m: SGD 110 loss=0.006858\n",
      "\u001b[36mdownhill\u001b[0m: validation 22 loss=0.006806 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 111 loss=0.006812\n",
      "\u001b[36mdownhill\u001b[0m: SGD 112 loss=0.006788\n",
      "\u001b[36mdownhill\u001b[0m: SGD 113 loss=0.006753\n",
      "\u001b[36mdownhill\u001b[0m: SGD 114 loss=0.006731\n",
      "\u001b[36mdownhill\u001b[0m: SGD 115 loss=0.006698\n",
      "\u001b[36mdownhill\u001b[0m: validation 23 loss=0.006656 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 116 loss=0.006676\n",
      "\u001b[36mdownhill\u001b[0m: SGD 117 loss=0.006632\n",
      "\u001b[36mdownhill\u001b[0m: SGD 118 loss=0.006593\n",
      "\u001b[36mdownhill\u001b[0m: SGD 119 loss=0.006571\n",
      "\u001b[36mdownhill\u001b[0m: SGD 120 loss=0.006547\n",
      "\u001b[36mdownhill\u001b[0m: validation 24 loss=0.006520 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 121 loss=0.006529\n",
      "\u001b[36mdownhill\u001b[0m: SGD 122 loss=0.006506\n",
      "\u001b[36mdownhill\u001b[0m: SGD 123 loss=0.006483\n",
      "\u001b[36mdownhill\u001b[0m: SGD 124 loss=0.006454\n",
      "\u001b[36mdownhill\u001b[0m: SGD 125 loss=0.006427\n",
      "\u001b[36mdownhill\u001b[0m: validation 25 loss=0.006405 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 126 loss=0.006393\n",
      "\u001b[36mdownhill\u001b[0m: SGD 127 loss=0.006365\n",
      "\u001b[36mdownhill\u001b[0m: SGD 128 loss=0.006341\n",
      "\u001b[36mdownhill\u001b[0m: SGD 129 loss=0.006310\n",
      "\u001b[36mdownhill\u001b[0m: SGD 130 loss=0.006302\n",
      "\u001b[36mdownhill\u001b[0m: validation 26 loss=0.006278 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 131 loss=0.006272\n",
      "\u001b[36mdownhill\u001b[0m: SGD 132 loss=0.006242\n",
      "\u001b[36mdownhill\u001b[0m: SGD 133 loss=0.006225\n",
      "\u001b[36mdownhill\u001b[0m: SGD 134 loss=0.006196\n",
      "\u001b[36mdownhill\u001b[0m: SGD 135 loss=0.006177\n",
      "\u001b[36mdownhill\u001b[0m: validation 27 loss=0.006151 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 136 loss=0.006150\n",
      "\u001b[36mdownhill\u001b[0m: SGD 137 loss=0.006140\n",
      "\u001b[36mdownhill\u001b[0m: SGD 138 loss=0.006115\n",
      "\u001b[36mdownhill\u001b[0m: SGD 139 loss=0.006096\n",
      "\u001b[36mdownhill\u001b[0m: SGD 140 loss=0.006070\n",
      "\u001b[36mdownhill\u001b[0m: validation 28 loss=0.006047 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 141 loss=0.006058\n",
      "\u001b[36mdownhill\u001b[0m: SGD 142 loss=0.006028\n",
      "\u001b[36mdownhill\u001b[0m: SGD 143 loss=0.006002\n",
      "\u001b[36mdownhill\u001b[0m: SGD 144 loss=0.005979\n",
      "\u001b[36mdownhill\u001b[0m: SGD 145 loss=0.005952\n",
      "\u001b[36mdownhill\u001b[0m: validation 29 loss=0.005950 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 146 loss=0.005941\n",
      "\u001b[36mdownhill\u001b[0m: SGD 147 loss=0.005920\n",
      "\u001b[36mdownhill\u001b[0m: SGD 148 loss=0.005915\n",
      "\u001b[36mdownhill\u001b[0m: SGD 149 loss=0.005891\n",
      "\u001b[36mdownhill\u001b[0m: SGD 150 loss=0.005873\n",
      "\u001b[36mdownhill\u001b[0m: validation 30 loss=0.005829 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 151 loss=0.005847\n",
      "\u001b[36mdownhill\u001b[0m: SGD 152 loss=0.005832\n",
      "\u001b[36mdownhill\u001b[0m: SGD 153 loss=0.005813\n",
      "\u001b[36mdownhill\u001b[0m: SGD 154 loss=0.005802\n",
      "\u001b[36mdownhill\u001b[0m: SGD 155 loss=0.005776\n",
      "\u001b[36mdownhill\u001b[0m: validation 31 loss=0.005739 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 156 loss=0.005759\n",
      "\u001b[36mdownhill\u001b[0m: SGD 157 loss=0.005741\n",
      "\u001b[36mdownhill\u001b[0m: SGD 158 loss=0.005726\n",
      "\u001b[36mdownhill\u001b[0m: SGD 159 loss=0.005697\n",
      "\u001b[36mdownhill\u001b[0m: SGD 160 loss=0.005690\n",
      "\u001b[36mdownhill\u001b[0m: validation 32 loss=0.005674 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 161 loss=0.005685\n",
      "\u001b[36mdownhill\u001b[0m: SGD 162 loss=0.005650\n",
      "\u001b[36mdownhill\u001b[0m: SGD 163 loss=0.005633\n",
      "\u001b[36mdownhill\u001b[0m: SGD 164 loss=0.005633\n",
      "\u001b[36mdownhill\u001b[0m: SGD 165 loss=0.005612\n",
      "\u001b[36mdownhill\u001b[0m: validation 33 loss=0.005560 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 166 loss=0.005602\n",
      "\u001b[36mdownhill\u001b[0m: SGD 167 loss=0.005568\n",
      "\u001b[36mdownhill\u001b[0m: SGD 168 loss=0.005565\n",
      "\u001b[36mdownhill\u001b[0m: SGD 169 loss=0.005540\n",
      "\u001b[36mdownhill\u001b[0m: SGD 170 loss=0.005542\n",
      "\u001b[36mdownhill\u001b[0m: validation 34 loss=0.005525 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 171 loss=0.005527\n",
      "\u001b[36mdownhill\u001b[0m: SGD 172 loss=0.005512\n",
      "\u001b[36mdownhill\u001b[0m: SGD 173 loss=0.005494\n",
      "\u001b[36mdownhill\u001b[0m: SGD 174 loss=0.005463\n",
      "\u001b[36mdownhill\u001b[0m: SGD 175 loss=0.005481\n",
      "\u001b[36mdownhill\u001b[0m: validation 35 loss=0.005447 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 176 loss=0.005455\n",
      "\u001b[36mdownhill\u001b[0m: SGD 177 loss=0.005441\n",
      "\u001b[36mdownhill\u001b[0m: SGD 178 loss=0.005429\n",
      "\u001b[36mdownhill\u001b[0m: SGD 179 loss=0.005401\n",
      "\u001b[36mdownhill\u001b[0m: SGD 180 loss=0.005397\n",
      "\u001b[36mdownhill\u001b[0m: validation 36 loss=0.005363 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 181 loss=0.005378\n",
      "\u001b[36mdownhill\u001b[0m: SGD 182 loss=0.005376\n",
      "\u001b[36mdownhill\u001b[0m: SGD 183 loss=0.005357\n",
      "\u001b[36mdownhill\u001b[0m: SGD 184 loss=0.005344\n",
      "\u001b[36mdownhill\u001b[0m: SGD 185 loss=0.005341\n",
      "\u001b[36mdownhill\u001b[0m: validation 37 loss=0.005315 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 186 loss=0.005318\n",
      "\u001b[36mdownhill\u001b[0m: SGD 187 loss=0.005294\n",
      "\u001b[36mdownhill\u001b[0m: SGD 188 loss=0.005300\n",
      "\u001b[36mdownhill\u001b[0m: SGD 189 loss=0.005271\n",
      "\u001b[36mdownhill\u001b[0m: SGD 190 loss=0.005277\n",
      "\u001b[36mdownhill\u001b[0m: validation 38 loss=0.005249 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 191 loss=0.005255\n",
      "\u001b[36mdownhill\u001b[0m: SGD 192 loss=0.005234\n",
      "\u001b[36mdownhill\u001b[0m: SGD 193 loss=0.005237\n",
      "\u001b[36mdownhill\u001b[0m: SGD 194 loss=0.005216\n",
      "\u001b[36mdownhill\u001b[0m: SGD 195 loss=0.005204\n",
      "\u001b[36mdownhill\u001b[0m: validation 39 loss=0.005182 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 196 loss=0.005193\n",
      "\u001b[36mdownhill\u001b[0m: SGD 197 loss=0.005197\n",
      "\u001b[36mdownhill\u001b[0m: SGD 198 loss=0.005170\n",
      "\u001b[36mdownhill\u001b[0m: SGD 199 loss=0.005147\n",
      "\u001b[36mdownhill\u001b[0m: SGD 200 loss=0.005167\n",
      "dataset4: 551 of 551 mini-batches from (55021, 49)\n",
      "dataset5: 1 of 1 mini-batches from (13755, 49)\n",
      "\u001b[36mdownhill\u001b[0m: compiling evaluation function\n",
      "\u001b[36mdownhill\u001b[0m: compiling \u001b[31mSGD\u001b[0m optimizer\n",
      "\u001b[36mdownhill\u001b[0m: setting: patience\u001b[0m = \u001b[33m100\u001b[0m\n",
      "\u001b[36mdownhill\u001b[0m: setting: validate_every\u001b[0m = \u001b[33m5\u001b[0m\n",
      "\u001b[36mdownhill\u001b[0m: setting: min_improvement\u001b[0m = \u001b[33m0.001\u001b[0m\n",
      "\u001b[36mdownhill\u001b[0m: setting: max_gradient_norm\u001b[0m = \u001b[33m0\u001b[0m\n",
      "\u001b[36mdownhill\u001b[0m: setting: max_gradient_elem\u001b[0m = \u001b[33m0\u001b[0m\n",
      "\u001b[36mdownhill\u001b[0m: setting: learning_rate\u001b[0m = \u001b[33mTensorConstant{0.01}\u001b[0m\n",
      "\u001b[36mdownhill\u001b[0m: setting: momentum\u001b[0m = \u001b[33m0.0\u001b[0m\n",
      "\u001b[36mdownhill\u001b[0m: setting: nesterov\u001b[0m = \u001b[33mFalse\u001b[0m\n",
      "\u001b[36mdownhill\u001b[0m: validation 0 loss=0.304710 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 1 loss=0.200030\n",
      "\u001b[36mdownhill\u001b[0m: SGD 2 loss=0.089441\n",
      "\u001b[36mdownhill\u001b[0m: SGD 3 loss=0.070977\n",
      "\u001b[36mdownhill\u001b[0m: SGD 4 loss=0.065204\n",
      "\u001b[36mdownhill\u001b[0m: SGD 5 loss=0.061046\n",
      "\u001b[36mdownhill\u001b[0m: validation 1 loss=0.059850 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 6 loss=0.057304\n",
      "\u001b[36mdownhill\u001b[0m: SGD 7 loss=0.053773\n",
      "\u001b[36mdownhill\u001b[0m: SGD 8 loss=0.050586\n",
      "\u001b[36mdownhill\u001b[0m: SGD 9 loss=0.047584\n",
      "\u001b[36mdownhill\u001b[0m: SGD 10 loss=0.044819\n",
      "\u001b[36mdownhill\u001b[0m: validation 2 loss=0.043964 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 11 loss=0.042311\n",
      "\u001b[36mdownhill\u001b[0m: SGD 12 loss=0.040034\n",
      "\u001b[36mdownhill\u001b[0m: SGD 13 loss=0.037949\n",
      "\u001b[36mdownhill\u001b[0m: SGD 14 loss=0.036063\n",
      "\u001b[36mdownhill\u001b[0m: SGD 15 loss=0.034380\n",
      "\u001b[36mdownhill\u001b[0m: validation 3 loss=0.033771 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 16 loss=0.032862\n",
      "\u001b[36mdownhill\u001b[0m: SGD 17 loss=0.031527\n",
      "\u001b[36mdownhill\u001b[0m: SGD 18 loss=0.030315\n",
      "\u001b[36mdownhill\u001b[0m: SGD 19 loss=0.029212\n",
      "\u001b[36mdownhill\u001b[0m: SGD 20 loss=0.028244\n",
      "\u001b[36mdownhill\u001b[0m: validation 4 loss=0.027781 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 21 loss=0.027391\n",
      "\u001b[36mdownhill\u001b[0m: SGD 22 loss=0.026598\n",
      "\u001b[36mdownhill\u001b[0m: SGD 23 loss=0.025872\n",
      "\u001b[36mdownhill\u001b[0m: SGD 24 loss=0.025216\n",
      "\u001b[36mdownhill\u001b[0m: SGD 25 loss=0.024625\n",
      "\u001b[36mdownhill\u001b[0m: validation 5 loss=0.024201 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 26 loss=0.024058\n",
      "\u001b[36mdownhill\u001b[0m: SGD 27 loss=0.023533\n",
      "\u001b[36mdownhill\u001b[0m: SGD 28 loss=0.023078\n",
      "\u001b[36mdownhill\u001b[0m: SGD 29 loss=0.022613\n",
      "\u001b[36mdownhill\u001b[0m: SGD 30 loss=0.022188\n",
      "\u001b[36mdownhill\u001b[0m: validation 6 loss=0.021855 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 31 loss=0.021812\n",
      "\u001b[36mdownhill\u001b[0m: SGD 32 loss=0.021396\n",
      "\u001b[36mdownhill\u001b[0m: SGD 33 loss=0.021074\n",
      "\u001b[36mdownhill\u001b[0m: SGD 34 loss=0.020712\n",
      "\u001b[36mdownhill\u001b[0m: SGD 35 loss=0.020406\n",
      "\u001b[36mdownhill\u001b[0m: validation 7 loss=0.020132 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 36 loss=0.020080\n",
      "\u001b[36mdownhill\u001b[0m: SGD 37 loss=0.019809\n",
      "\u001b[36mdownhill\u001b[0m: SGD 38 loss=0.019523\n",
      "\u001b[36mdownhill\u001b[0m: SGD 39 loss=0.019240\n",
      "\u001b[36mdownhill\u001b[0m: SGD 40 loss=0.018980\n",
      "\u001b[36mdownhill\u001b[0m: validation 8 loss=0.018705 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 41 loss=0.018763\n",
      "\u001b[36mdownhill\u001b[0m: SGD 42 loss=0.018489\n",
      "\u001b[36mdownhill\u001b[0m: SGD 43 loss=0.018306\n",
      "\u001b[36mdownhill\u001b[0m: SGD 44 loss=0.018053\n",
      "\u001b[36mdownhill\u001b[0m: SGD 45 loss=0.017850\n",
      "\u001b[36mdownhill\u001b[0m: validation 9 loss=0.017587 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 46 loss=0.017652\n",
      "\u001b[36mdownhill\u001b[0m: SGD 47 loss=0.017419\n",
      "\u001b[36mdownhill\u001b[0m: SGD 48 loss=0.017233\n",
      "\u001b[36mdownhill\u001b[0m: SGD 49 loss=0.017026\n",
      "\u001b[36mdownhill\u001b[0m: SGD 50 loss=0.016866\n",
      "\u001b[36mdownhill\u001b[0m: validation 10 loss=0.016570 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 51 loss=0.016660\n",
      "\u001b[36mdownhill\u001b[0m: SGD 52 loss=0.016495\n",
      "\u001b[36mdownhill\u001b[0m: SGD 53 loss=0.016335\n",
      "\u001b[36mdownhill\u001b[0m: SGD 54 loss=0.016181\n",
      "\u001b[36mdownhill\u001b[0m: SGD 55 loss=0.016024\n",
      "\u001b[36mdownhill\u001b[0m: validation 11 loss=0.015813 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 56 loss=0.015862\n",
      "\u001b[36mdownhill\u001b[0m: SGD 57 loss=0.015718\n",
      "\u001b[36mdownhill\u001b[0m: SGD 58 loss=0.015554\n",
      "\u001b[36mdownhill\u001b[0m: SGD 59 loss=0.015439\n",
      "\u001b[36mdownhill\u001b[0m: SGD 60 loss=0.015265\n",
      "\u001b[36mdownhill\u001b[0m: validation 12 loss=0.015067 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 61 loss=0.015145\n",
      "\u001b[36mdownhill\u001b[0m: SGD 62 loss=0.015019\n",
      "\u001b[36mdownhill\u001b[0m: SGD 63 loss=0.014884\n",
      "\u001b[36mdownhill\u001b[0m: SGD 64 loss=0.014742\n",
      "\u001b[36mdownhill\u001b[0m: SGD 65 loss=0.014630\n",
      "\u001b[36mdownhill\u001b[0m: validation 13 loss=0.014398 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 66 loss=0.014507\n",
      "\u001b[36mdownhill\u001b[0m: SGD 67 loss=0.014403\n",
      "\u001b[36mdownhill\u001b[0m: SGD 68 loss=0.014282\n",
      "\u001b[36mdownhill\u001b[0m: SGD 69 loss=0.014173\n",
      "\u001b[36mdownhill\u001b[0m: SGD 70 loss=0.014067\n",
      "\u001b[36mdownhill\u001b[0m: validation 14 loss=0.013835 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 71 loss=0.013941\n",
      "\u001b[36mdownhill\u001b[0m: SGD 72 loss=0.013828\n",
      "\u001b[36mdownhill\u001b[0m: SGD 73 loss=0.013740\n",
      "\u001b[36mdownhill\u001b[0m: SGD 74 loss=0.013654\n",
      "\u001b[36mdownhill\u001b[0m: SGD 75 loss=0.013547\n",
      "\u001b[36mdownhill\u001b[0m: validation 15 loss=0.013327 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 76 loss=0.013441\n",
      "\u001b[36mdownhill\u001b[0m: SGD 77 loss=0.013351\n",
      "\u001b[36mdownhill\u001b[0m: SGD 78 loss=0.013264\n",
      "\u001b[36mdownhill\u001b[0m: SGD 79 loss=0.013166\n",
      "\u001b[36mdownhill\u001b[0m: SGD 80 loss=0.013100\n",
      "\u001b[36mdownhill\u001b[0m: validation 16 loss=0.012885 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 81 loss=0.013006\n",
      "\u001b[36mdownhill\u001b[0m: SGD 82 loss=0.012932\n",
      "\u001b[36mdownhill\u001b[0m: SGD 83 loss=0.012838\n",
      "\u001b[36mdownhill\u001b[0m: SGD 84 loss=0.012745\n",
      "\u001b[36mdownhill\u001b[0m: SGD 85 loss=0.012690\n",
      "\u001b[36mdownhill\u001b[0m: validation 17 loss=0.012512 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 86 loss=0.012607\n",
      "\u001b[36mdownhill\u001b[0m: SGD 87 loss=0.012514\n",
      "\u001b[36mdownhill\u001b[0m: SGD 88 loss=0.012462\n",
      "\u001b[36mdownhill\u001b[0m: SGD 89 loss=0.012382\n",
      "\u001b[36mdownhill\u001b[0m: SGD 90 loss=0.012286\n",
      "\u001b[36mdownhill\u001b[0m: validation 18 loss=0.012122 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 91 loss=0.012245\n",
      "\u001b[36mdownhill\u001b[0m: SGD 92 loss=0.012158\n",
      "\u001b[36mdownhill\u001b[0m: SGD 93 loss=0.012109\n",
      "\u001b[36mdownhill\u001b[0m: SGD 94 loss=0.012062\n",
      "\u001b[36mdownhill\u001b[0m: SGD 95 loss=0.011977\n",
      "\u001b[36mdownhill\u001b[0m: validation 19 loss=0.011804 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 96 loss=0.011930\n",
      "\u001b[36mdownhill\u001b[0m: SGD 97 loss=0.011863\n",
      "\u001b[36mdownhill\u001b[0m: SGD 98 loss=0.011801\n",
      "\u001b[36mdownhill\u001b[0m: SGD 99 loss=0.011745\n",
      "\u001b[36mdownhill\u001b[0m: SGD 100 loss=0.011683\n",
      "\u001b[36mdownhill\u001b[0m: validation 20 loss=0.011530 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 101 loss=0.011618\n",
      "\u001b[36mdownhill\u001b[0m: SGD 102 loss=0.011579\n",
      "\u001b[36mdownhill\u001b[0m: SGD 103 loss=0.011514\n",
      "\u001b[36mdownhill\u001b[0m: SGD 104 loss=0.011463\n",
      "\u001b[36mdownhill\u001b[0m: SGD 105 loss=0.011412\n",
      "\u001b[36mdownhill\u001b[0m: validation 21 loss=0.011315 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 106 loss=0.011375\n",
      "\u001b[36mdownhill\u001b[0m: SGD 107 loss=0.011308\n",
      "\u001b[36mdownhill\u001b[0m: SGD 108 loss=0.011258\n",
      "\u001b[36mdownhill\u001b[0m: SGD 109 loss=0.011210\n",
      "\u001b[36mdownhill\u001b[0m: SGD 110 loss=0.011175\n",
      "\u001b[36mdownhill\u001b[0m: validation 22 loss=0.011043 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 111 loss=0.011115\n",
      "\u001b[36mdownhill\u001b[0m: SGD 112 loss=0.011077\n",
      "\u001b[36mdownhill\u001b[0m: SGD 113 loss=0.011039\n",
      "\u001b[36mdownhill\u001b[0m: SGD 114 loss=0.010972\n",
      "\u001b[36mdownhill\u001b[0m: SGD 115 loss=0.010939\n",
      "\u001b[36mdownhill\u001b[0m: validation 23 loss=0.010785 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 116 loss=0.010909\n",
      "\u001b[36mdownhill\u001b[0m: SGD 117 loss=0.010860\n",
      "\u001b[36mdownhill\u001b[0m: SGD 118 loss=0.010832\n",
      "\u001b[36mdownhill\u001b[0m: SGD 119 loss=0.010793\n",
      "\u001b[36mdownhill\u001b[0m: SGD 120 loss=0.010726\n",
      "\u001b[36mdownhill\u001b[0m: validation 24 loss=0.010602 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 121 loss=0.010736\n",
      "\u001b[36mdownhill\u001b[0m: SGD 122 loss=0.010691\n",
      "\u001b[36mdownhill\u001b[0m: SGD 123 loss=0.010634\n",
      "\u001b[36mdownhill\u001b[0m: SGD 124 loss=0.010590\n",
      "\u001b[36mdownhill\u001b[0m: SGD 125 loss=0.010586\n",
      "\u001b[36mdownhill\u001b[0m: validation 25 loss=0.010431 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 126 loss=0.010525\n",
      "\u001b[36mdownhill\u001b[0m: SGD 127 loss=0.010515\n",
      "\u001b[36mdownhill\u001b[0m: SGD 128 loss=0.010459\n",
      "\u001b[36mdownhill\u001b[0m: SGD 129 loss=0.010424\n",
      "\u001b[36mdownhill\u001b[0m: SGD 130 loss=0.010401\n",
      "\u001b[36mdownhill\u001b[0m: validation 26 loss=0.010296 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 131 loss=0.010374\n",
      "\u001b[36mdownhill\u001b[0m: SGD 132 loss=0.010362\n",
      "\u001b[36mdownhill\u001b[0m: SGD 133 loss=0.010300\n",
      "\u001b[36mdownhill\u001b[0m: SGD 134 loss=0.010291\n",
      "\u001b[36mdownhill\u001b[0m: SGD 135 loss=0.010254\n",
      "\u001b[36mdownhill\u001b[0m: validation 27 loss=0.010106 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 136 loss=0.010218\n",
      "\u001b[36mdownhill\u001b[0m: SGD 137 loss=0.010198\n",
      "\u001b[36mdownhill\u001b[0m: SGD 138 loss=0.010184\n",
      "\u001b[36mdownhill\u001b[0m: SGD 139 loss=0.010138\n",
      "\u001b[36mdownhill\u001b[0m: SGD 140 loss=0.010109\n",
      "\u001b[36mdownhill\u001b[0m: validation 28 loss=0.009998 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 141 loss=0.010098\n",
      "\u001b[36mdownhill\u001b[0m: SGD 142 loss=0.010074\n",
      "\u001b[36mdownhill\u001b[0m: SGD 143 loss=0.010050\n",
      "\u001b[36mdownhill\u001b[0m: SGD 144 loss=0.010000\n",
      "\u001b[36mdownhill\u001b[0m: SGD 145 loss=0.009990\n",
      "\u001b[36mdownhill\u001b[0m: validation 29 loss=0.009901 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 146 loss=0.009965\n",
      "\u001b[36mdownhill\u001b[0m: SGD 147 loss=0.009937\n",
      "\u001b[36mdownhill\u001b[0m: SGD 148 loss=0.009919\n",
      "\u001b[36mdownhill\u001b[0m: SGD 149 loss=0.009920\n",
      "\u001b[36mdownhill\u001b[0m: SGD 150 loss=0.009880\n",
      "\u001b[36mdownhill\u001b[0m: validation 30 loss=0.009762 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 151 loss=0.009850\n",
      "\u001b[36mdownhill\u001b[0m: SGD 152 loss=0.009828\n",
      "\u001b[36mdownhill\u001b[0m: SGD 153 loss=0.009798\n",
      "\u001b[36mdownhill\u001b[0m: SGD 154 loss=0.009798\n",
      "\u001b[36mdownhill\u001b[0m: SGD 155 loss=0.009760\n",
      "\u001b[36mdownhill\u001b[0m: validation 31 loss=0.009684 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 156 loss=0.009734\n",
      "\u001b[36mdownhill\u001b[0m: SGD 157 loss=0.009714\n",
      "\u001b[36mdownhill\u001b[0m: SGD 158 loss=0.009701\n",
      "\u001b[36mdownhill\u001b[0m: SGD 159 loss=0.009684\n",
      "\u001b[36mdownhill\u001b[0m: SGD 160 loss=0.009657\n",
      "\u001b[36mdownhill\u001b[0m: validation 32 loss=0.009595 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 161 loss=0.009643\n",
      "\u001b[36mdownhill\u001b[0m: SGD 162 loss=0.009637\n",
      "\u001b[36mdownhill\u001b[0m: SGD 163 loss=0.009609\n",
      "\u001b[36mdownhill\u001b[0m: SGD 164 loss=0.009588\n",
      "\u001b[36mdownhill\u001b[0m: SGD 165 loss=0.009563\n",
      "\u001b[36mdownhill\u001b[0m: validation 33 loss=0.009478 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 166 loss=0.009549\n",
      "\u001b[36mdownhill\u001b[0m: SGD 167 loss=0.009528\n",
      "\u001b[36mdownhill\u001b[0m: SGD 168 loss=0.009503\n",
      "\u001b[36mdownhill\u001b[0m: SGD 169 loss=0.009480\n",
      "\u001b[36mdownhill\u001b[0m: SGD 170 loss=0.009485\n",
      "\u001b[36mdownhill\u001b[0m: validation 34 loss=0.009363 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 171 loss=0.009459\n",
      "\u001b[36mdownhill\u001b[0m: SGD 172 loss=0.009454\n",
      "\u001b[36mdownhill\u001b[0m: SGD 173 loss=0.009425\n",
      "\u001b[36mdownhill\u001b[0m: SGD 174 loss=0.009426\n",
      "\u001b[36mdownhill\u001b[0m: SGD 175 loss=0.009368\n",
      "\u001b[36mdownhill\u001b[0m: validation 35 loss=0.009312 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 176 loss=0.009394\n",
      "\u001b[36mdownhill\u001b[0m: SGD 177 loss=0.009370\n",
      "\u001b[36mdownhill\u001b[0m: SGD 178 loss=0.009365\n",
      "\u001b[36mdownhill\u001b[0m: SGD 179 loss=0.009337\n",
      "\u001b[36mdownhill\u001b[0m: SGD 180 loss=0.009332\n",
      "\u001b[36mdownhill\u001b[0m: validation 36 loss=0.009254 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 181 loss=0.009312\n",
      "\u001b[36mdownhill\u001b[0m: SGD 182 loss=0.009270\n",
      "\u001b[36mdownhill\u001b[0m: SGD 183 loss=0.009287\n",
      "\u001b[36mdownhill\u001b[0m: SGD 184 loss=0.009266\n",
      "\u001b[36mdownhill\u001b[0m: SGD 185 loss=0.009255\n",
      "\u001b[36mdownhill\u001b[0m: validation 37 loss=0.009160 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 186 loss=0.009243\n",
      "\u001b[36mdownhill\u001b[0m: SGD 187 loss=0.009220\n",
      "\u001b[36mdownhill\u001b[0m: SGD 188 loss=0.009206\n",
      "\u001b[36mdownhill\u001b[0m: SGD 189 loss=0.009205\n",
      "\u001b[36mdownhill\u001b[0m: SGD 190 loss=0.009180\n",
      "\u001b[36mdownhill\u001b[0m: validation 38 loss=0.009101 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 191 loss=0.009159\n",
      "\u001b[36mdownhill\u001b[0m: SGD 192 loss=0.009148\n",
      "\u001b[36mdownhill\u001b[0m: SGD 193 loss=0.009146\n",
      "\u001b[36mdownhill\u001b[0m: SGD 194 loss=0.009121\n",
      "\u001b[36mdownhill\u001b[0m: SGD 195 loss=0.009119\n",
      "\u001b[36mdownhill\u001b[0m: validation 39 loss=0.009033 *\n",
      "\u001b[36mdownhill\u001b[0m: SGD 196 loss=0.009104\n",
      "\u001b[36mdownhill\u001b[0m: SGD 197 loss=0.009096\n",
      "\u001b[36mdownhill\u001b[0m: SGD 198 loss=0.009088\n",
      "\u001b[36mdownhill\u001b[0m: SGD 199 loss=0.009076\n",
      "\u001b[36mdownhill\u001b[0m: SGD 200 loss=0.009058\n",
      " + Stopping epoch: [200. 200. 200.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/zhongying/lr/NewTest/Plot_Curves.py:203: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  ax = plt.subplot(111)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset6: 551 of 551 mini-batches from (55021, 63)\n",
      "dataset7: 1 of 1 mini-batches from (13755, 63)\n",
      "\u001b[36mdownhill\u001b[0m: compiling evaluation function\n",
      "\u001b[36mdownhill\u001b[0m: compiling \u001b[31mADADELTA\u001b[0m optimizer\n",
      "\u001b[36mdownhill\u001b[0m: setting: rms_halflife\u001b[0m = \u001b[33m14\u001b[0m\n",
      "\u001b[36mdownhill\u001b[0m: setting: rms_regularizer\u001b[0m = \u001b[33m1e-08\u001b[0m\n",
      "\u001b[36mdownhill\u001b[0m: setting: patience\u001b[0m = \u001b[33m1\u001b[0m\n",
      "\u001b[36mdownhill\u001b[0m: setting: validate_every\u001b[0m = \u001b[33m5\u001b[0m\n",
      "\u001b[36mdownhill\u001b[0m: setting: min_improvement\u001b[0m = \u001b[33m0.001\u001b[0m\n",
      "\u001b[36mdownhill\u001b[0m: setting: max_gradient_norm\u001b[0m = \u001b[33m0\u001b[0m\n",
      "\u001b[36mdownhill\u001b[0m: setting: max_gradient_elem\u001b[0m = \u001b[33m0\u001b[0m\n",
      "\u001b[36mdownhill\u001b[0m: setting: learning_rate\u001b[0m = \u001b[33mTensorConstant{0.0001}\u001b[0m\n",
      "\u001b[36mdownhill\u001b[0m: setting: momentum\u001b[0m = \u001b[33m0.0\u001b[0m\n",
      "\u001b[36mdownhill\u001b[0m: setting: nesterov\u001b[0m = \u001b[33mFalse\u001b[0m\n",
      "\u001b[36mdownhill\u001b[0m: validation 0 loss=0.014176 *\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 1 loss=0.010302\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 2 loss=0.008113\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 3 loss=0.006987\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 4 loss=0.006281\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 5 loss=0.005748\n",
      "\u001b[36mdownhill\u001b[0m: validation 1 loss=0.005502 *\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 6 loss=0.005326\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 7 loss=0.004991\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 8 loss=0.004734\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 9 loss=0.004532\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 10 loss=0.004362\n",
      "\u001b[36mdownhill\u001b[0m: validation 2 loss=0.004240 *\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 11 loss=0.004211\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 12 loss=0.004070\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 13 loss=0.003938\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 14 loss=0.003826\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 15 loss=0.003712\n",
      "\u001b[36mdownhill\u001b[0m: validation 3 loss=0.003627 *\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 16 loss=0.003611\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 17 loss=0.003521\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 18 loss=0.003447\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 19 loss=0.003372\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 20 loss=0.003305\n",
      "\u001b[36mdownhill\u001b[0m: validation 4 loss=0.003233 *\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 21 loss=0.003236\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 22 loss=0.003175\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 23 loss=0.003102\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 24 loss=0.003061\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 25 loss=0.003012\n",
      "\u001b[36mdownhill\u001b[0m: validation 5 loss=0.002923 *\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 26 loss=0.002939\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 27 loss=0.002884\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 28 loss=0.002843\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 29 loss=0.002790\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 30 loss=0.002728\n",
      "\u001b[36mdownhill\u001b[0m: validation 6 loss=0.002648 *\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 31 loss=0.002689\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 32 loss=0.002638\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 33 loss=0.002623\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 34 loss=0.002566\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 35 loss=0.002519\n",
      "\u001b[36mdownhill\u001b[0m: validation 7 loss=0.002507 *\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 36 loss=0.002495\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 37 loss=0.002448\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 38 loss=0.002421\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 39 loss=0.002397\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 40 loss=0.002356\n",
      "\u001b[36mdownhill\u001b[0m: validation 8 loss=0.002299 *\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 41 loss=0.002337\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 42 loss=0.002301\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 43 loss=0.002276\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 44 loss=0.002252\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 45 loss=0.002242\n",
      "\u001b[36mdownhill\u001b[0m: validation 9 loss=0.002190 *\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 46 loss=0.002211\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 47 loss=0.002175\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 48 loss=0.002156\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 49 loss=0.002128\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 50 loss=0.002123\n",
      "\u001b[36mdownhill\u001b[0m: validation 10 loss=0.002073 *\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 51 loss=0.002098\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 52 loss=0.002069\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 53 loss=0.002060\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 54 loss=0.002034\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 55 loss=0.002021\n",
      "\u001b[36mdownhill\u001b[0m: validation 11 loss=0.001953 *\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 56 loss=0.002014\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 57 loss=0.001980\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 58 loss=0.001984\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 59 loss=0.001962\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 60 loss=0.001937\n",
      "\u001b[36mdownhill\u001b[0m: validation 12 loss=0.001885 *\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 61 loss=0.001928\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 62 loss=0.001912\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 63 loss=0.001892\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 64 loss=0.001883\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 65 loss=0.001859\n",
      "\u001b[36mdownhill\u001b[0m: validation 13 loss=0.001796 *\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 66 loss=0.001858\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 67 loss=0.001832\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 68 loss=0.001824\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 69 loss=0.001808\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 70 loss=0.001800\n",
      "\u001b[36mdownhill\u001b[0m: validation 14 loss=0.001767 *\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 71 loss=0.001784\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 72 loss=0.001779\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 73 loss=0.001747\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 74 loss=0.001742\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 75 loss=0.001734\n",
      "\u001b[36mdownhill\u001b[0m: validation 15 loss=0.001676 *\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 76 loss=0.001727\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 77 loss=0.001713\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 78 loss=0.001693\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 79 loss=0.001684\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 80 loss=0.001682\n",
      "\u001b[36mdownhill\u001b[0m: validation 16 loss=0.001605 *\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 81 loss=0.001666\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 82 loss=0.001657\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 83 loss=0.001644\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 84 loss=0.001638\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 85 loss=0.001615\n",
      "\u001b[36mdownhill\u001b[0m: validation 17 loss=0.001590 *\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 86 loss=0.001608\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 87 loss=0.001606\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 88 loss=0.001603\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 89 loss=0.001581\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 90 loss=0.001571\n",
      "\u001b[36mdownhill\u001b[0m: validation 18 loss=0.001543 *\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 91 loss=0.001572\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 92 loss=0.001561\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 93 loss=0.001548\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 94 loss=0.001536\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 95 loss=0.001539\n",
      "\u001b[36mdownhill\u001b[0m: validation 19 loss=0.001482 *\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 96 loss=0.001529\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 97 loss=0.001511\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 98 loss=0.001518\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 99 loss=0.001498\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 100 loss=0.001492\n",
      "\u001b[36mdownhill\u001b[0m: validation 20 loss=0.001432 *\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 101 loss=0.001485\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 102 loss=0.001480\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 103 loss=0.001462\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 104 loss=0.001464\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 105 loss=0.001452\n",
      "\u001b[36mdownhill\u001b[0m: validation 21 loss=0.001423 *\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 106 loss=0.001445\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 107 loss=0.001438\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 108 loss=0.001434\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 109 loss=0.001424\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 110 loss=0.001413\n",
      "\u001b[36mdownhill\u001b[0m: validation 22 loss=0.001392 *\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 111 loss=0.001405\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 112 loss=0.001407\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 113 loss=0.001387\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 114 loss=0.001397\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 115 loss=0.001382\n",
      "\u001b[36mdownhill\u001b[0m: validation 23 loss=0.001426\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 116 loss=0.001379\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 117 loss=0.001381\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 118 loss=0.001363\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 119 loss=0.001358\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 120 loss=0.001364\n",
      "\u001b[36mdownhill\u001b[0m: validation 24 loss=0.001334 *\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 121 loss=0.001349\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 122 loss=0.001337\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 123 loss=0.001339\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 124 loss=0.001332\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 125 loss=0.001326\n",
      "\u001b[36mdownhill\u001b[0m: validation 25 loss=0.001270 *\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 126 loss=0.001320\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 127 loss=0.001310\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 128 loss=0.001312\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 129 loss=0.001307\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 130 loss=0.001302\n",
      "\u001b[36mdownhill\u001b[0m: validation 26 loss=0.001307\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 131 loss=0.001301\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 132 loss=0.001293\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 133 loss=0.001291\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 134 loss=0.001279\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 135 loss=0.001278\n",
      "\u001b[36mdownhill\u001b[0m: validation 27 loss=0.001234 *\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 136 loss=0.001272\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 137 loss=0.001263\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 138 loss=0.001256\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 139 loss=0.001254\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 140 loss=0.001249\n",
      "\u001b[36mdownhill\u001b[0m: validation 28 loss=0.001192 *\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 141 loss=0.001239\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 142 loss=0.001253\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 143 loss=0.001248\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 144 loss=0.001234\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 145 loss=0.001229\n",
      "\u001b[36mdownhill\u001b[0m: validation 29 loss=0.001230\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 146 loss=0.001221\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 147 loss=0.001219\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 148 loss=0.001218\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 149 loss=0.001207\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 150 loss=0.001206\n",
      "\u001b[36mdownhill\u001b[0m: validation 30 loss=0.001160 *\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 151 loss=0.001205\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 152 loss=0.001211\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 153 loss=0.001201\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 154 loss=0.001190\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 155 loss=0.001181\n",
      "\u001b[36mdownhill\u001b[0m: validation 31 loss=0.001167\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 156 loss=0.001184\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 157 loss=0.001181\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 158 loss=0.001177\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 159 loss=0.001166\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 160 loss=0.001164\n",
      "\u001b[36mdownhill\u001b[0m: validation 32 loss=0.001097 *\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 161 loss=0.001163\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 162 loss=0.001157\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 163 loss=0.001155\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 164 loss=0.001146\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 165 loss=0.001148\n",
      "\u001b[36mdownhill\u001b[0m: validation 33 loss=0.001134\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 166 loss=0.001133\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 167 loss=0.001141\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 168 loss=0.001130\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 169 loss=0.001132\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 170 loss=0.001119\n",
      "\u001b[36mdownhill\u001b[0m: validation 34 loss=0.001114\n",
      "\u001b[36mdownhill\u001b[0m: patience elapsed!\n",
      "cen\n",
      "AUC 0.4577077698953164\n",
      "Precision 0.876892727660482\n",
      "Recall 0.9564989822622856\n",
      "f1_score 0.9149675947817864\n",
      "TPR 0.9564989822622856\n",
      "FPR 0.82287954383464\n",
      "KDE AE/DensityBasedOneClassClassifier\n",
      "AUC 0.4490936735284773\n",
      "Precision 0.8704951272156453\n",
      "Recall 0.7740040709508578\n",
      "f1_score 0.8194187907893117\n",
      "TPR 0.7740040709508578\n",
      "FPR 0.7056307911617962\n",
      "SVM05\n",
      "AUC 0.45262747524983327\n",
      "Precision 0.8756910613607751\n",
      "Recall 0.9303867403314917\n",
      "f1_score 0.9022106925332731\n",
      "TPR 0.9303867403314917\n",
      "FPR 0.8093371347113328\n",
      "SVM01\n",
      "AUC 0.43095941339509053\n",
      "Precision 0.9333333333333333\n",
      "Recall 0.03338179703402152\n",
      "f1_score 0.06445816956765862\n",
      "TPR 0.03338179703402152\n",
      "FPR 0.014611546685673557\n"
     ]
    }
   ],
   "source": [
    "sda, re = train_SdAE(pre_lr       = 1e-2,             \n",
    "                    end2end_lr   = 1e-4,\n",
    "                    algo         = 'adadelta',\n",
    "                    dataset      = datasets,\n",
    "                    data_name    = data,\n",
    "                    n_validate   = n_vali,\n",
    "                    norm         = norm,\n",
    "                    batch_size   = batch,\n",
    "                    hidden_sizes = h_sizes,\n",
    "                    corruptions  = corruptions,\n",
    "                    patience     = pat,\n",
    "                    validation   = val)\n",
    "\n",
    "\n",
    "#*******Computer AUC on hidden data*************\n",
    "lof,cen,dis,kde,svm05,svm01,ae  = sda.Compute_AUC_Hidden(train_X, test_X, actual, norm, data)\n",
    "auc_hidden = np.column_stack([batch, re[0], lof, cen, dis, kde, svm05, svm01, ae , 100*re[2]])\n",
    "AUC_Hidden = np.append(AUC_Hidden, auc_hidden)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    LOF    CEN    MDIS   KDE   SVM5    SVM1    AE    RE*100\n",
      "[[0.277 0.689 0.687 0.684 0.356 0.112 0.976 0.031]]\n"
     ]
    }
   ],
   "source": [
    "AUC_Hidden  =  np.reshape(AUC_Hidden, (-1, 10))\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "column_list = [2,3,4,5,6,7,8,9]\n",
    "print(\"    LOF    CEN    MDIS   KDE   SVM5    SVM1    AE    RE*100\")\n",
    "print (AUC_Hidden[:,column_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    LOF    CEN    MDIS   KDE   SVM5    SVM1    AE    RE*100\n",
      "[[0.277 0.689 0.687 0.684 0.356 0.112 0.976 0.031]]\n"
     ]
    }
   ],
   "source": [
    "AUC_Hidden  =  np.reshape(AUC_Hidden, (-1, 10))\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "column_list = [2,3,4,5,6,7,8,9]\n",
    "print(\"    LOF    CEN    MDIS   KDE   SVM5    SVM1    AE    RE*100\")\n",
    "print (AUC_Hidden[:,column_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
