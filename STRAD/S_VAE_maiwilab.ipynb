{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.configdefaults): install mkl with `conda install mkl-service`: No module named 'mkl'\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import sys\n",
    "import numpy\n",
    "import numpy as np\n",
    "import downhill\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "from mlp import HiddenLayer\n",
    "from dA import dA\n",
    "#from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "\n",
    "from ProcessingData import load_data, normalize_data\n",
    "from Methods import auc_density, auc_AEbased\n",
    "from Plot_Curves import Plotting_End2End_RE, Plotting_Pre_RE, Plotting_AUC_RE, Plotting_Pre_RE1\n",
    "from Plot_Curves import Plotting_AUC_Batch_Size, Plotting_Monitor, plot_auc_size_input, visualize_hidden\n",
    "from Plot_Curves import visualize_hidden1, histogram_z, Plotting_Loss_Component, plot_auc_size_2\n",
    "from nnet_architecture import hyper_parameters\n",
    "from stopping_para import stopping_para_vae\n",
    "\n",
    "\n",
    "path = \"./Results/Exp_Hidden/\"\n",
    "\n",
    "\"Check whether weights matrix is updated or not\"\n",
    "def check_weight_update(sda):\n",
    "    np.set_printoptions(precision=4, suppress=True)\n",
    "    \"Check whether weights matrix is updated or not\"\n",
    "    for i in range(sda.n_layers):\n",
    "        print(\"\\n %d\" %i)\n",
    "        print (sda.Autoencoder_layers[i].W.get_value(borrow=True))\n",
    "\n",
    "    for j in range(sda.n_layers, 2*sda.n_layers):\n",
    "        print(\"\\n %d\" % j)\n",
    "        print (sda.Autoencoder_layers[j].W.eval())\n",
    "\n",
    "    print (\"\\n ************************************ \")\n",
    "\n",
    "\n",
    "\n",
    "class SdA(object):\n",
    "\n",
    "    def __init__(self, numpy_rng, theano_rng=None, n_ins=100, hidden_layers_sizes=[50, 30]):\n",
    "\n",
    "        self.encoder = []\n",
    "        self.decoder = []\n",
    "        self.params  = []\n",
    "        self.n_layers = len(hidden_layers_sizes)\n",
    "\n",
    "        self.rng = theano.tensor.shared_randomstreams.RandomStreams(numpy_rng.randint(2 ** 30))\n",
    "\n",
    "        assert self.n_layers > 0\n",
    "\n",
    "        self.x = T.matrix('x')\n",
    "\n",
    "        for i in range(self.n_layers):\n",
    "            # the size of the input is either the number of hidden units of\n",
    "            # the layer below or the input size if we are on the first layer\n",
    "            if (i == 0):\n",
    "                input_size  = n_ins\n",
    "                layer_input = self.x\n",
    "            else:\n",
    "                input_size  = hidden_layers_sizes[i - 1]\n",
    "                layer_input = self.encoder[-1].output\n",
    "\n",
    "            \"Not the middle hidden layer\"\n",
    "            if (i < self.n_layers-1):\n",
    "                encoder_layer = HiddenLayer(rng = numpy_rng,\n",
    "                                    input       = layer_input,\n",
    "                                    n_in        = input_size,\n",
    "                                    n_out       = hidden_layers_sizes[i],\n",
    "                                    activation  = T.tanh)\n",
    "                self.encoder.append(encoder_layer)\n",
    "                self.params.extend(encoder_layer.params)\n",
    "\n",
    "            else:\n",
    "                \"The middle hidden layer, linear\"\n",
    "                encoder_mu = HiddenLayer(rng = numpy_rng,\n",
    "                                    input    = self.encoder[i-1].output,\n",
    "                                    n_in     = input_size,\n",
    "                                    n_out    = hidden_layers_sizes[i])\n",
    "\n",
    "                encoder_var = HiddenLayer(rng= numpy_rng,\n",
    "                                    input    = self.encoder[i-1].output,\n",
    "                                    n_in     = input_size,\n",
    "                                    n_out    = hidden_layers_sizes[i])\n",
    "\n",
    "                self.encoder.append(encoder_mu)\n",
    "                self.params.extend(encoder_mu.params)\n",
    "\n",
    "                self.encoder.append(encoder_var)\n",
    "                self.params.extend(encoder_var.params)\n",
    "\n",
    "\n",
    "        \"*************** Sample z **************\"\n",
    "        mu       = self.encoder[-2].output\n",
    "        log_var  = self.encoder[-1].output\n",
    "        sample_z = self.sample_z(mu, log_var)\n",
    "\n",
    "        \"*************** Decoder ***************\"\n",
    "        i = self.n_layers-1\n",
    "        while (i >=0):\n",
    "            input_size = hidden_layers_sizes[i]\n",
    "            if ( i > 0):\n",
    "                output_size = hidden_layers_sizes[i-1]\n",
    "            else:\n",
    "                output_size =  n_ins\n",
    "\n",
    "            if (i==self.n_layers-1): \n",
    "                layer_input = sample_z\n",
    "                decoder_layer = HiddenLayer(rng = numpy_rng,\n",
    "                                        input   = layer_input,\n",
    "                                        n_in    = input_size,\n",
    "                                        n_out   = output_size,\n",
    "                                        activation = T.tanh)  \n",
    "                self.decoder.append(decoder_layer)\n",
    "                self.params.extend(decoder_layer.params)\n",
    "            else:\n",
    "                layer_input = self.decoder[-1].output\n",
    "                decoder_layer = HiddenLayer(rng = numpy_rng,\n",
    "                                        input   = layer_input,\n",
    "                                        n_in    = input_size,\n",
    "                                        n_out   = output_size,\n",
    "                                        activation = T.tanh,\n",
    "                                        W = self.encoder[i].W.T)\n",
    "\n",
    "                self.decoder.append(decoder_layer)\n",
    "                self.params.append(decoder_layer.b)\n",
    "            i = i - 1\n",
    "\n",
    "        \"******************* End To End Cost function ************************\"\n",
    "        z_mu  = self.encoder[-2].output\n",
    "        z_var = self.encoder[-1].output\n",
    "        z     = self.decoder[-1].output\n",
    "\n",
    "\n",
    "        self.recon = (((self.x - z)**2).mean(1)).mean()\n",
    "        \n",
    "        self.KL    = T.mean((0.5)*T.mean(T.exp(z_var) + z_mu**2 - 1 - z_var, 1))\n",
    "        \n",
    "        self.end2end_cost = self.recon + self.KL        \n",
    "\n",
    "    \"**************************** Sample z ***********************************\"\n",
    "    def sample_z(self, mu, log_var):\n",
    "        eps = self.rng.normal(mu.shape, 0.0, 1.0 , dtype = theano.config.floatX)\n",
    "        sample_z = mu + T.exp(log_var / 2) * eps\n",
    "        return sample_z\n",
    "    \n",
    "    \n",
    "    def Recon_KL_loss_batch(self, train_x, batch_size):\n",
    "\n",
    "        index = T.lscalar('index')\n",
    "        \n",
    "        batch_begin = index * batch_size\n",
    "       \n",
    "        batch_end = batch_begin + batch_size\n",
    "        KL1 = self.lamda*T.log10(self.KL+1)\n",
    "        loss_com = theano.function([index],\n",
    "                             outputs = [self.recon, KL1],\n",
    "                             givens={self.x: train_x[batch_begin : batch_end]})\n",
    "        return loss_com\n",
    "\n",
    "    def Recon_KL_Loss(self, train_x, batch_size):\n",
    "        n_train = train_x.get_value().shape[0]\n",
    "        n_batches = (int)(n_train/batch_size)\n",
    "        loss_com = self.Recon_KL_loss_batch(train_x, batch_size)\n",
    "        loss = np.empty([0,2])\n",
    "        for batch_index in range(n_batches):\n",
    "          l = loss_com(index = batch_index)\n",
    "          loss = np.append(loss, [l[0], l[1]])\n",
    "        loss = np.reshape(loss, (-1,2))\n",
    "\n",
    "        return (loss.mean(0))\n",
    "\n",
    "    \"************************** Get Mu and Log_var ****************************\"\n",
    "    def get_mu_logvar(self,data_set):\n",
    "        data_size = data_set.get_value().shape[0]\n",
    "        index   = T.lscalar('index')\n",
    "        mu      =  self.encoder[-2].output\n",
    "        log_var =  self.encoder[-1].output\n",
    "        mu_logvar = theano.function([index],\n",
    "                                    outputs = [mu, log_var],\n",
    "                                    givens={self.x: data_set[index : data_size]})\n",
    "        return mu_logvar(0)\n",
    "\n",
    "\n",
    "    \"****** Error on train_x and valid_x before optimization process **********\"\n",
    "    def Loss_train_valid(self, train_x, valid_x):\n",
    "        index = T.lscalar('index')\n",
    "\n",
    "        train_size = train_x.get_value().shape[0]\n",
    "        tm = theano.function([index],\n",
    "                             outputs = self.end2end_cost,\n",
    "                             givens={self.x: train_x[index : train_size]})\n",
    "\n",
    "        valid_size = valid_x.get_value().shape[0]\n",
    "        vm = theano.function([index],\n",
    "                             outputs = self.end2end_cost,\n",
    "                             givens={self.x: valid_x[index : valid_size]})\n",
    "\n",
    "        return tm(0), vm(0)\n",
    "\n",
    "    \"**************************** Get hidden data z **************************\"\n",
    "    def get_hidden_data(self,data_set):\n",
    "        data_size = data_set.get_value().shape[0]\n",
    "        index = T.lscalar('index')\n",
    "        mu      =  self.encoder[-2].output\n",
    "        log_var =  self.encoder[-1].output\n",
    "        z = self.sample_z(mu, log_var)\n",
    "        hidden_data = theano.function([index],\n",
    "                                      outputs = z,\n",
    "                                      givens={self.x: data_set[index : data_size]})\n",
    "        return hidden_data(0)\n",
    "\n",
    "\n",
    "    \"************get data from the output of Autoencoder**********************\"\n",
    "    def get_output_data(self,data_set):\n",
    "        data_size = data_set.get_value().shape[0]\n",
    "        index  = T.lscalar('index')\n",
    "        y_data = theano.function([index],\n",
    "                                      outputs = self.decoder[-1].output,\n",
    "                                      givens={self.x: data_set[index : data_size]})\n",
    "        return y_data(0)\n",
    "\n",
    "    \"******************** Histogram z, z_mu and z_var ************************\"\n",
    "    def Plot_histogram_z(self, train_set, test_set, actual, epoch, path):\n",
    "        z_train    = self.get_hidden_data(train_set)\n",
    "        mu, logvar = self.get_mu_logvar(train_set)\n",
    "\n",
    "        np.savetxt(path + \"Visualize_histogram/\" + \"z_train_\" + str(epoch) + \"_\" + str(self.alpha)+\".csv\", z_train, delimiter=\",\", fmt='%f' )\n",
    "\n",
    "        histogram_z(z_train[:,0],        'z'  , self.alpha, epoch, path)\n",
    "\n",
    "    \"********************* Compute AUC on hidden data *************************\"\n",
    "    def Compute_AUC_Hidden(self, train_set, test_set, actual, norm, data_name):\n",
    "        z_train = self.get_hidden_data(train_set)           #get hidden values\n",
    "        z_test  = self.get_hidden_data(test_set)            #get hidden values\n",
    "        y_test  = self.get_output_data(test_set)            #get prediction values\n",
    "        \"Compute performance of classifiers on latent data\"\n",
    "        lof, cen, dis, kde, svm05, svm01 = auc_density(z_train, z_test, actual, norm)\n",
    "        ae                               = auc_AEbased(test_set.get_value(), y_test, actual)\n",
    "        return lof, cen, dis, kde, svm05, svm01, ae\n",
    "\n",
    "    \"**************************************************************************\"\n",
    "    def Save_Hidden_Data(self, train_set, test_set, data_name, path):\n",
    "        z_train = self.get_hidden_data(train_set)      #get hidden values\n",
    "        z_test  = self.get_hidden_data(test_set)       #get hidden values\n",
    "        np.savetxt(path + data_name + \"_train_z.csv\", z_train, delimiter=\",\", fmt='%f' )\n",
    "        np.savetxt(path + data_name + \"_test_z.csv\" ,  z_test, delimiter=\",\", fmt='%f' )\n",
    "\n",
    "    \"******** Training End-to-End Early-stopping by Downhill Package *********\"\n",
    "    def End2end_Early_stopping(self, numpy_rng, dataset, n_validate, data_name,\n",
    "                               batch_size, end2end_lr, algo, norm, patience, validation):\n",
    "\n",
    "        train_X, test_X, actual = dataset\n",
    "        valid_x = train_X.get_value()[:n_validate]\n",
    "        train_x = train_X.get_value()[n_validate:]\n",
    "        \"for compute tm and vm before optimization process\"\n",
    "\n",
    "        \"Training network by downhill\"\n",
    "        #'adadelta' 'adagrad (default 0.01)' 'adam''esgd' 'nag''rmsprop' 'rprop' 'sgd'\n",
    "        opt = downhill.build(algo = algo, params= self.params, loss = self.end2end_cost, inputs = [self.x])\n",
    "        train = downhill.Dataset(train_x, batch_size = batch_size, rng = numpy_rng)\n",
    "        valid = downhill.Dataset(valid_x, batch_size = len(valid_x), rng = numpy_rng)\n",
    "\n",
    "        \"***** Monitor before optimization *****\"\n",
    "        stop_ep = 0\n",
    "        RE = np.empty([0,3])\n",
    "\n",
    "        for tm, vm in opt.iterate(train,                     # 5, 5, 1e-2, 0.9\n",
    "                                  valid,\n",
    "                                  patience = patience,               # 10\n",
    "                                  validate_every = validation,            # 5\n",
    "                                  min_improvement = 1e-3,       # 1e-3\n",
    "                                  #learning_rate =  end2end_lr,  # 1e-4\n",
    "                                  momentum = 0.0,\n",
    "                                  nesterov = False):\n",
    "            stop_ep = stop_ep+1\n",
    "\n",
    "            re = np.column_stack([stop_ep, vm['loss'], tm['loss']])\n",
    "            RE = np.append(RE, re)\n",
    "\n",
    "            if (stop_ep >= 1000):\n",
    "                break\n",
    "\n",
    "        RE = np.reshape(RE, (-1,3))\n",
    "        Plotting_End2End_RE(RE, stop_ep, 0.0, 0.4, data_name, path)\n",
    "        np.savetxt(path +  data_name + \"_training_error1.csv\", RE, delimiter=\",\", fmt='%f' )\n",
    "\n",
    "        np.set_printoptions(precision=6, suppress=True)\n",
    "        print (\"\\n \",RE[stop_ep-1])\n",
    "\n",
    "        return RE[stop_ep-1]\n",
    "\n",
    "\n",
    "\n",
    "def test_SdA(pre_lr=0.01, end2end_lr=1e-4, algo = 'sgd',\n",
    "             dataset=[], data_name = \"WBC\", n_validate = 0, norm = \"maxabs\",\n",
    "             batch_size=10, hidden_sizes = [1,1,1],\n",
    "             patience = 1, validation = 1):\n",
    "\n",
    "    numpy_rng = numpy.random.RandomState(89677)    \n",
    "    train_X, test_X, actual = dataset               \n",
    "\n",
    "    input_size = train_X.get_value().shape[1]      \n",
    "    train_x    = train_X.get_value()[n_validate:]   \n",
    "    n_train_batches   = train_x.shape[0]\n",
    "    n_train_batches //= batch_size                 \n",
    "\n",
    "    \n",
    "    sda = SdA(numpy_rng = numpy_rng, n_ins = input_size,\n",
    "              hidden_layers_sizes = hidden_sizes)\n",
    "\n",
    "\n",
    "    RE = sda.End2end_Early_stopping(numpy_rng, dataset, n_validate, data_name,\n",
    "                               batch_size, end2end_lr, algo, norm, patience, validation)\n",
    "    return sda, RE\n",
    "\n",
    "\n",
    "def Main_Test():\n",
    "\n",
    "    list_data =  [\"PageBlocks\", \"WPBC\", \"PenDigits\", \"GLASS\", \"Shuttle\", \"Arrhythmia\",\\\n",
    "                 \"CTU13_10\", \"CTU13_08\",\"CTU13_09\",\"CTU13_13\",\\\n",
    "                 \"Spambase\", \"UNSW\", \"NSLKDD\", \"InternetAds\"]\n",
    "    \n",
    "    norm         = \"maxabs\"           \n",
    "\n",
    "    print (\"+ Data: \", list_data)\n",
    "    print (\"+ Scaler: \", norm)\n",
    "\n",
    "    AUC_Hidden = np.empty([0,10])     \n",
    "    num = 0                          \n",
    "    for data in list_data:\n",
    "        num = num + 1\n",
    "\n",
    "        h_sizes = hyper_parameters(data)                  \n",
    "        train_set, test_set, actual = load_data(data)     \n",
    "\n",
    "        train_X, test_X = normalize_data(train_set, test_set, norm)  #Normalize data\n",
    "\n",
    "        train_X = theano.shared(numpy.asarray(train_X, dtype=theano.config.floatX), borrow=True)\n",
    "        test_X  = theano.shared(numpy.asarray(test_X,  dtype=theano.config.floatX), borrow=True)\n",
    "\n",
    "        datasets = [(train_X), (test_X), (actual)]          \n",
    "\n",
    "        in_dim   = train_set.shape[1]                       \n",
    "        n_vali   = (int)(train_set.shape[0]/5)              \n",
    "        n_train  = len(train_set) - n_vali                 \n",
    "        #batch    = int(n_train/20)                         \n",
    "                                                    \n",
    "        pat, val, batch, n_batch = stopping_para_vae(n_train)\n",
    "\n",
    "        print (\"\\n\" + str(num) + \".\", data, \"...\" )\n",
    "        print (\" + Hidden Sizes: \",in_dim, h_sizes, \"- Batch_sizes:\", batch)\n",
    "        print (\" + Data: %d (%d train, %d vali) - %d normal, %d anomaly\"\\\n",
    "            %(len(train_set), n_train, n_vali, \\\n",
    "            len(test_set[(actual == 1)]), len(test_set[(actual == 0)])))\n",
    "\n",
    "        print(\" + Patience: %5.0d, Validate: %5.0d,  \\n + Batch size: %5.0d, n batch:%5.0d\"\\\n",
    "             %(pat, val, batch, n_batch))\n",
    "\n",
    "                               \n",
    "        sda, re = test_SdA(pre_lr       = 1e-2,            \n",
    "                                   end2end_lr   = 1e-4,\n",
    "                                   algo         = 'adadelta',\n",
    "                                   dataset      = datasets,\n",
    "                                   data_name    = data,\n",
    "                                   n_validate   = n_vali,\n",
    "                                   norm         = norm,\n",
    "                                   batch_size   = batch,\n",
    "                                   hidden_sizes = h_sizes,\n",
    "                                   patience     = pat,\n",
    "                                   validation   = val)\n",
    "\n",
    "        #Computer AUC on hidden data\n",
    "        lof,cen,dis,kde,svm05,svm01,ae  = sda.Compute_AUC_Hidden(train_X, test_X, actual, norm, data)\n",
    "        auc_hidden = np.column_stack([batch, re[0], lof, cen, dis, kde, svm05, svm01, ae , 100*re[2]])\n",
    "        AUC_Hidden = np.append(AUC_Hidden, auc_hidden)\n",
    "\n",
    "        sda.Save_Hidden_Data(train_X, test_X, data, path)\n",
    "\n",
    "        AUC_Hidden  =  np.reshape(AUC_Hidden, (-1, 10))\n",
    "        np.set_printoptions(precision=3, suppress=True)\n",
    "        column_list = [2,3,4,5,6,7,8,9]\n",
    "        print(\"    LOF    CEN    MDIS   KDE   SVM5    SVM1    AE    RE*100\")\n",
    "        print (AUC_Hidden[:,column_list])\n",
    "        print(\"\\n\")\n",
    "        \n",
    "    AUC_Hidden  =  np.reshape(AUC_Hidden, (-1, 10))\n",
    "    np.set_printoptions(precision=3, suppress=True)\n",
    "    column_list = [2,3,4,5,6,7,8,9]\n",
    "    print(\"    LOF    CEN    MDIS   KDE   SVM5    SVM1    AE    RE*100\")\n",
    "    print (AUC_Hidden[:,column_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE\n",
      "+ Scaler:  maxabs\n",
      "+ Corruptions:  [0.1, 0.1, 0.1]\n",
      "\n",
      "1. maiwilab ...\n",
      " + Hidden Sizes:  63 [85, 49, 12] - Batch_sizes: 100\n",
      " + Data: 68776 (55021 train, 13755 vali) - test: 17195 normal, 2806 anomaly\n",
      " + Patience:     4, Validate:     1,  \n",
      " + Batch size:   100, n batch:  550\n",
      "dataset0: 551 of 551 mini-batches from (55021, 63)\n",
      "dataset1: 1 of 1 mini-batches from (13755, 63)\n",
      "\u001b[36mdownhill\u001b[0m: compiling evaluation function\n",
      "\u001b[36mdownhill\u001b[0m: compiling \u001b[31mADADELTA\u001b[0m optimizer\n",
      "\u001b[36mdownhill\u001b[0m: setting: rms_halflife\u001b[0m = \u001b[33m14\u001b[0m\n",
      "\u001b[36mdownhill\u001b[0m: setting: rms_regularizer\u001b[0m = \u001b[33m1e-08\u001b[0m\n",
      "\u001b[36mdownhill\u001b[0m: setting: patience\u001b[0m = \u001b[33m4\u001b[0m\n",
      "\u001b[36mdownhill\u001b[0m: setting: validate_every\u001b[0m = \u001b[33m1\u001b[0m\n",
      "\u001b[36mdownhill\u001b[0m: setting: min_improvement\u001b[0m = \u001b[33m0.001\u001b[0m\n",
      "\u001b[36mdownhill\u001b[0m: setting: max_gradient_norm\u001b[0m = \u001b[33m0\u001b[0m\n",
      "\u001b[36mdownhill\u001b[0m: setting: max_gradient_elem\u001b[0m = \u001b[33m0\u001b[0m\n",
      "\u001b[36mdownhill\u001b[0m: setting: learning_rate\u001b[0m = \u001b[33mTensorConstant{0.0001}\u001b[0m\n",
      "\u001b[36mdownhill\u001b[0m: setting: momentum\u001b[0m = \u001b[33m0.0\u001b[0m\n",
      "\u001b[36mdownhill\u001b[0m: setting: nesterov\u001b[0m = \u001b[33mFalse\u001b[0m\n",
      "\u001b[36mdownhill\u001b[0m: validation 0 loss=0.387713 *\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 1 loss=0.099645\n",
      "\u001b[36mdownhill\u001b[0m: validation 1 loss=0.066837 *\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 2 loss=0.065884\n",
      "\u001b[36mdownhill\u001b[0m: validation 2 loss=0.066037 *\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 3 loss=0.065297\n",
      "\u001b[36mdownhill\u001b[0m: validation 3 loss=0.065540 *\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 4 loss=0.064988\n",
      "\u001b[36mdownhill\u001b[0m: validation 4 loss=0.065275 *\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 5 loss=0.064798\n",
      "\u001b[36mdownhill\u001b[0m: validation 5 loss=0.065293\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 6 loss=0.064716\n",
      "\u001b[36mdownhill\u001b[0m: validation 6 loss=0.065229\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 7 loss=0.064658\n",
      "\u001b[36mdownhill\u001b[0m: validation 7 loss=0.065121 *\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 8 loss=0.064600\n",
      "\u001b[36mdownhill\u001b[0m: validation 8 loss=0.065186\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 9 loss=0.064580\n",
      "\u001b[36mdownhill\u001b[0m: validation 9 loss=0.065170\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 10 loss=0.064532\n",
      "\u001b[36mdownhill\u001b[0m: validation 10 loss=0.064931 *\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 11 loss=0.064516\n",
      "\u001b[36mdownhill\u001b[0m: validation 11 loss=0.065043\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 12 loss=0.064541\n",
      "\u001b[36mdownhill\u001b[0m: validation 12 loss=0.065030\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 13 loss=0.064504\n",
      "\u001b[36mdownhill\u001b[0m: validation 13 loss=0.065045\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 14 loss=0.064543\n",
      "\u001b[36mdownhill\u001b[0m: validation 14 loss=0.064964\n",
      "\u001b[36mdownhill\u001b[0m: ADADELTA 15 loss=0.064510\n",
      "\u001b[36mdownhill\u001b[0m: validation 15 loss=0.065015\n",
      "\u001b[36mdownhill\u001b[0m: patience elapsed!\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  [15.        0.064964  0.06451 ]\n",
      "cen\n",
      "AUC 0.4980982678043995\n",
      "Precision 0.8614084507042253\n",
      "Recall 0.533527188136086\n",
      "f1_score 0.6589333812174537\n",
      "TPR 0.533527188136086\n",
      "FPR 0.5260156806842481\n",
      "KDE AE/DensityBasedOneClassClassifier\n",
      "AUC 0.49755962641429885\n",
      "Precision 0.8615602836879432\n",
      "Recall 0.5298633323640594\n",
      "f1_score 0.6561757292041772\n",
      "TPR 0.5298633323640594\n",
      "FPR 0.5217391304347826\n",
      "SVM05\n",
      "AUC 0.4975606419758102\n",
      "Precision 0.8608999879358186\n",
      "Recall 0.8300087234661239\n",
      "f1_score 0.8451721789595238\n",
      "TPR 0.8300087234661239\n",
      "FPR 0.8218104062722738\n",
      "SVM01\n",
      "AUC 0.4974881847708468\n",
      "Precision 0.8614669323465225\n",
      "Recall 0.528002326257633\n",
      "f1_score 0.6547198384654215\n",
      "TPR 0.528002326257633\n",
      "FPR 0.5203136136849608\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy\n",
    "norm         = \"maxabs\"                \n",
    "corruptions  = [0.1, 0.1, 0.1]\n",
    "\n",
    "print (\"VAE\")\n",
    "#print (\"+ Data: \", list_data)\n",
    "print (\"+ Scaler: \", norm)\n",
    "print (\"+ Corruptions: \", corruptions)\n",
    "\n",
    "AUC_Hidden = np.empty([0,10])     \n",
    "\n",
    "num = 0\n",
    "data=\"maiwilab\"\n",
    "num = num + 1\n",
    "h_sizes =[85, 49, 12] #hyper_parameters(data)                   \n",
    "Xtrain=np.load('IDS_x_train.npy')\n",
    "Xtest=np.load('IDS_x_test.npy')\n",
    "total=100000\n",
    "x=np.concatenate((Xtrain,Xtest),axis=0)[:total]\n",
    "\n",
    "\n",
    "Ytrain=np.load('IDS_y_train.npy')\n",
    "Ytest=np.load('IDS_y_test.npy')\n",
    "label=np.concatenate((Ytrain,Ytest),axis=0)[:total]\n",
    "x1=pd.DataFrame(x)\n",
    "#actual=np.array([0 if i==1 else 1 for i in label])\n",
    "x1['label']=label\n",
    "d=x1.values\n",
    "d = d[~np.isnan(d).any(axis=1)]    #discard the '?' values\n",
    "seed=0\n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(d)\n",
    "\n",
    "dX = d[:,0:-1]              #put data to dX without the last column (labels)\n",
    "dy = d[:,-1].astype(int)                #put label to dy\n",
    "dy = dy > 0\n",
    "\n",
    "                            # dy=True with anomaly labels\n",
    "    \n",
    "                            # separate into normal and anomaly\n",
    "dX0 = dX[~dy]               # Normal data \n",
    "dX1 = dX[dy]                # Anomaly data\n",
    "dy0 = dy[~dy]               # Normal label\n",
    "dy1 = dy[dy]                # Anomaly label\n",
    "\n",
    "#print(\"Normal: %d Anomaly %d\" %(len(dX0), len(dX1)))\n",
    "split = 0.8             #split 80% for training, 20% for testing\n",
    "\n",
    "idx0  = int(split * len(dX0))\n",
    "idx1  = int(split * len(dX1))\n",
    "\n",
    "train_set = dX0[:idx0]        # train_X is 80% of the normal class\n",
    "\n",
    "# test set is the other half of the normal class and all of the anomaly class\n",
    "test_set = np.concatenate((dX0[idx0:], dX1[idx1:]))  # 30% of normal and 30% of anomaly\n",
    "test_y = np.concatenate((dy0[idx0:], dy1[idx1:]))  # 30% of normal and 30% of anomaly label\n",
    "#conver test_y into 1 or 0 for computing AUC later\n",
    "actual = (~test_y).astype(np.int)\n",
    "\n",
    "train_X, test_X = normalize_data(train_set, test_set, norm)\n",
    "train_X = theano.shared(numpy.asarray(train_X, dtype=theano.config.floatX), borrow=True)\n",
    "test_X  = theano.shared(numpy.asarray(test_X,  dtype=theano.config.floatX), borrow=True)\n",
    "\n",
    "datasets = [(train_X), (test_X), (actual)]          \n",
    "\n",
    "in_dim   = train_set.shape[1]                       \n",
    "n_vali   = (int)(train_set.shape[0]/5)              \n",
    "n_train  = len(train_set) - n_vali                  \n",
    "#batch     = int(n_train/20)                          \n",
    "\n",
    "pat, val, batch, n_batch = stopping_para_vae(n_train)\n",
    "\n",
    "\n",
    "print (\"\\n\" + str(num) + \".\", data, \"...\" )\n",
    "print (\" + Hidden Sizes: \",in_dim, h_sizes, \"- Batch_sizes:\", batch)\n",
    "print (\" + Data: %d (%d train, %d vali) - test: %d normal, %d anomaly\"\\\n",
    "%(len(train_set), n_train, n_vali, \\\n",
    "len(test_set[(actual == 1)]), len(test_set[(actual == 0)])))\n",
    "print(\" + Patience: %5.0d, Validate: %5.0d,  \\n + Batch size: %5.0d, n batch:%5.0d\"\\\n",
    "%(pat, val, batch, n_batch))\n",
    "sda, re = test_SdA(pre_lr       = 1e-2,            \n",
    "                   end2end_lr   = 1e-4,\n",
    "                   algo         = 'adadelta',\n",
    "                   dataset      = datasets,\n",
    "                   data_name    = data,\n",
    "                   n_validate   = n_vali,\n",
    "                   norm         = norm,\n",
    "                   batch_size   = batch,\n",
    "                   hidden_sizes = h_sizes,\n",
    "                   patience     = pat,\n",
    "                   validation   = val)\n",
    "\n",
    "#Computer AUC on hidden data\n",
    "lof,cen,dis,kde,svm05,svm01,ae  = sda.Compute_AUC_Hidden(train_X, test_X, actual, norm, data)\n",
    "auc_hidden = np.column_stack([batch, re[0], lof, cen, dis, kde, svm05, svm01, ae , 100*re[2]])\n",
    "AUC_Hidden = np.append(AUC_Hidden, auc_hidden)\n",
    "\n",
    "sda.Save_Hidden_Data(train_X, test_X, data, path)\n",
    "\n",
    "AUC_Hidden  =  np.reshape(AUC_Hidden, (-1, 10))\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "column_list = [2,3,4,5,6,7,8,9]\n",
    "print(\"    LOF    CEN    MDIS   KDE   SVM5    SVM1    AE    RE*100\")\n",
    "print (AUC_Hidden[:,column_list])\n",
    "print(\"\\n\")\n",
    "\n",
    "AUC_Hidden  =  np.reshape(AUC_Hidden, (-1, 10))\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "column_list = [2,3,4,5,6,7,8,9]\n",
    "print(\"    LOF    CEN    MDIS   KDE   SVM5    SVM1    AE    RE*100\")\n",
    "print (AUC_Hidden[:,column_list])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:lr] *",
   "language": "python",
   "name": "conda-env-lr-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
