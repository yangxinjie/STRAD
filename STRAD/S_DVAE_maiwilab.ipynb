{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.configdefaults): install mkl with `conda install mkl-service`: No module named 'mkl'\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import downhill\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy\n",
    "import pandas as pd\n",
    "from mlp import HiddenLayer\n",
    "from dA import dA\n",
    "#from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "\n",
    "from ProcessingData import load_data, normalize_data\n",
    "from Methods import auc_density, auc_AEbased\n",
    "from Plot_Curves import Plotting_End2End_RE, Plotting_Pre_RE, Plotting_AUC_RE, Plotting_Pre_RE1\n",
    "from Plot_Curves import Plotting_AUC_Batch_Size, Plotting_Monitor, plot_auc_size_input, visualize_hidden\n",
    "from Plot_Curves import visualize_hidden1, histogram_z, Plotting_Loss_Component, plot_auc_size_2\n",
    "from nnet_architecture import hyper_parameters\n",
    "from stopping_para import stopping_para_vae\n",
    "\n",
    "path = \"./Results/\"\n",
    "\n",
    "\"Check whether weights matrix is updated or not\"\n",
    "def check_weight_update(sda):\n",
    "    np.set_printoptions(precision=4, suppress=True)\n",
    "    \"Check whether weights matrix is updated or not\"\n",
    "    for i in range(sda.n_layers):\n",
    "        print(\"\\n %d\" %i)\n",
    "        print (sda.Autoencoder_layers[i].W.get_value(borrow=True))\n",
    "\n",
    "    for j in range(sda.n_layers, 2*sda.n_layers):\n",
    "        print(\"\\n %d\" % j)\n",
    "        print (sda.Autoencoder_layers[j].W.eval())\n",
    "\n",
    "    print (\"\\n ************************************ \")\n",
    "\n",
    "\n",
    "\n",
    "class SdA(object):\n",
    "\n",
    "    def __init__(self, numpy_rng, theano_rng=None, n_ins=100, hidden_layers_sizes=[50, 30]):\n",
    "\n",
    "        self.encoder = []\n",
    "        self.decoder = []\n",
    "        self.params  = []\n",
    "        self.n_layers = len(hidden_layers_sizes)\n",
    "\n",
    "        \"\"\"set seed one time per dataset, rng will randomly generate different\n",
    "        number in each mini_batch_size and, different in each epoch. This is\n",
    "        repetive the same number when we create new SdA object. See theano_rondom.py\"\"\"\n",
    "        self.rng = theano.tensor.shared_randomstreams.RandomStreams(numpy_rng.randint(2 ** 30))\n",
    "\n",
    "        assert self.n_layers > 0\n",
    "#        if not theano_rng:\n",
    "#            theano_rng = RandomStreams(numpy_rng.randint(2 ** 30))\n",
    "\n",
    "        # allocate symbolic variables for the data\n",
    "        # the data is presented as rasterized images\n",
    "        self.x = T.matrix('x')\n",
    "\n",
    "        for i in range(self.n_layers):\n",
    "            # the size of the input is either the number of hidden units of\n",
    "            # the layer below or the input size if we are on the first layer\n",
    "            if (i == 0):\n",
    "                input_size  = n_ins\n",
    "                layer_input = self.x\n",
    "            else:\n",
    "                input_size  = hidden_layers_sizes[i - 1]\n",
    "                layer_input = self.encoder[-1].output\n",
    "\n",
    "            \"Not the middle hidden layer\"\n",
    "            if (i < self.n_layers-1):\n",
    "                encoder_layer = HiddenLayer(rng = numpy_rng,\n",
    "                                    input       = layer_input,\n",
    "                                    n_in        = input_size,\n",
    "                                    n_out       = hidden_layers_sizes[i],\n",
    "                                    activation  = T.tanh)\n",
    "                self.encoder.append(encoder_layer)\n",
    "                self.params.extend(encoder_layer.params)\n",
    "\n",
    "            else:\n",
    "                \"The middle hidden layer, linear\"\n",
    "                encoder_mu = HiddenLayer(rng = numpy_rng,\n",
    "                                    input    = self.encoder[i-1].output,\n",
    "                                    n_in     = input_size,\n",
    "                                    n_out    = hidden_layers_sizes[i])\n",
    "\n",
    "                encoder_var = HiddenLayer(rng= numpy_rng,\n",
    "                                    input    = self.encoder[i-1].output,\n",
    "                                    n_in     = input_size,\n",
    "                                    n_out    = hidden_layers_sizes[i])\n",
    "\n",
    "                self.encoder.append(encoder_mu)\n",
    "                self.params.extend(encoder_mu.params)\n",
    "\n",
    "                self.encoder.append(encoder_var)\n",
    "                self.params.extend(encoder_var.params)\n",
    "\n",
    "\n",
    "        \"*************** Sample z **************\"\n",
    "        mu       = self.encoder[-2].output\n",
    "        log_var  = self.encoder[-1].output\n",
    "        sample_z = self.sample_z(mu, log_var)\n",
    "\n",
    "        \"*************** Decoder ***************\"\n",
    "        i = self.n_layers-1\n",
    "        while (i >=0):\n",
    "            input_size = hidden_layers_sizes[i]\n",
    "            if ( i > 0):\n",
    "                output_size = hidden_layers_sizes[i-1]\n",
    "            else:\n",
    "                output_size =  n_ins\n",
    "\n",
    "            if (i==self.n_layers-1):  #the first layer in decoder\n",
    "                layer_input = sample_z\n",
    "                decoder_layer = HiddenLayer(rng = numpy_rng,\n",
    "                                        input   = layer_input,\n",
    "                                        n_in    = input_size,\n",
    "                                        n_out   = output_size,\n",
    "                                        activation = T.tanh)   #may be linear\n",
    "                self.decoder.append(decoder_layer)\n",
    "                self.params.extend(decoder_layer.params)\n",
    "            else:\n",
    "                layer_input = self.decoder[-1].output\n",
    "                decoder_layer = HiddenLayer(rng = numpy_rng,\n",
    "                                        input   = layer_input,\n",
    "                                        n_in    = input_size,\n",
    "                                        n_out   = output_size,\n",
    "                                        activation = T.tanh,\n",
    "                                        W = self.encoder[i].W.T)\n",
    "\n",
    "                self.decoder.append(decoder_layer)\n",
    "                self.params.append(decoder_layer.b)\n",
    "            i = i - 1\n",
    "\n",
    "        \"******************* End To End Cost function ************************\"\n",
    "        z_mu  = self.encoder[-2].output\n",
    "        z_var = self.encoder[-1].output\n",
    "        y     = self.decoder[-1].output\n",
    "\n",
    "        self.alpha = 1e-8\n",
    "        self.lamda = 0.05\n",
    "        \n",
    "        self.recon = (((self.x - y)**2).mean(1)).mean()\n",
    "        \"\"\"When compute a constant together with theano variable, it will be converted\n",
    "        into the same shape as the theano variable. We may compute mean over features\n",
    "        of each example instead of sum. This is to avoid the difference in dimension of\n",
    "        each data. Default lamda = 0.05, alpha = 1e-8\"\"\"\n",
    "\n",
    "        alpha = self.alpha\n",
    "        self.KL    = T.mean((0.5/alpha)*T.mean(T.exp(z_var) + z_mu**2 - alpha - alpha * z_var + alpha*T.log(alpha), 1))\n",
    "        self.end2end_cost = self.recon + self.lamda*T.log10(self.KL+1)\n",
    "        \n",
    "        #Experiment: lamda = 0.05; alpha = 1e-8    1\n",
    "        #mean(1) is within example, mean(0) is within each feature\n",
    "        \n",
    "    \"**************************** Sample z ***********************************\"\n",
    "    def sample_z(self, mu, log_var):\n",
    "        eps = self.rng.normal(mu.shape, 0.0, 1.0 , dtype = theano.config.floatX)\n",
    "        sample_z = mu + T.exp(log_var / 2) * eps\n",
    "        return sample_z\n",
    "\n",
    "    \"********************** Compute KL and Recon Loss *************************\"\n",
    "#    def Recon_KL_Loss(self, data_set):\n",
    "#        data_size = data_set.get_value().shape[0]\n",
    "#        index = T.lscalar('index')\n",
    "#        KL1 = self.lamda*T.log10(self.KL+1)\n",
    "#        Loss = theano.function([index],\n",
    "#                               outputs = [self.recon, KL1],\n",
    "#                               givens={self.x: data_set[index : data_size]})\n",
    "#        return Loss(0)\n",
    "\n",
    "\n",
    "\n",
    "    def Recon_KL_loss_batch(self, train_x, batch_size):\n",
    "\n",
    "        index = T.lscalar('index')\n",
    "        # begining of a batch, given `index`\n",
    "        batch_begin = index * batch_size\n",
    "        # ending of a batch given `index`\n",
    "        batch_end = batch_begin + batch_size\n",
    "        KL1 = self.lamda*T.log10(self.KL+1)\n",
    "        loss_com = theano.function([index],\n",
    "                             outputs = [self.recon, KL1],\n",
    "                             givens={self.x: train_x[batch_begin : batch_end]})\n",
    "        return loss_com\n",
    "\n",
    "    def Recon_KL_Loss(self, train_x, batch_size):\n",
    "        n_train = train_x.get_value().shape[0]\n",
    "        n_batches = (int)(n_train/batch_size)\n",
    "        loss_com = self.Recon_KL_loss_batch(train_x, batch_size)\n",
    "        loss = np.empty([0,2])\n",
    "        for batch_index in range(n_batches):\n",
    "          l = loss_com(index = batch_index)\n",
    "          loss = np.append(loss, [l[0], l[1]])\n",
    "        loss = np.reshape(loss, (-1,2))\n",
    "\n",
    "        return (loss.mean(0))\n",
    "\n",
    "    \"************************** Get Mu and Log_var ****************************\"\n",
    "    def get_mu_logvar(self,data_set):\n",
    "        data_size = data_set.get_value().shape[0]\n",
    "        index   = T.lscalar('index')\n",
    "        mu      =  self.encoder[-2].output\n",
    "        log_var =  self.encoder[-1].output\n",
    "        mu_logvar = theano.function([index],\n",
    "                                    outputs = [mu, log_var],\n",
    "                                    givens={self.x: data_set[index : data_size]})\n",
    "        return mu_logvar(0)\n",
    "\n",
    "\n",
    "    \"****** Error on train_x and valid_x before optimization process **********\"\n",
    "    def Loss_train_valid(self, train_x, valid_x):\n",
    "        index = T.lscalar('index')\n",
    "\n",
    "        train_size = train_x.get_value().shape[0]\n",
    "        tm = theano.function([index],\n",
    "                             outputs = self.end2end_cost,\n",
    "                             givens={self.x: train_x[index : train_size]})\n",
    "\n",
    "        valid_size = valid_x.get_value().shape[0]\n",
    "        vm = theano.function([index],\n",
    "                             outputs = self.end2end_cost,\n",
    "                             givens={self.x: valid_x[index : valid_size]})\n",
    "\n",
    "        return tm(0), vm(0)\n",
    "\n",
    "    \"**************************** Get hidden data z **************************\"\n",
    "    def get_hidden_data(self,data_set):\n",
    "        data_size = data_set.get_value().shape[0]\n",
    "        index = T.lscalar('index')\n",
    "        mu      =  self.encoder[-2].output\n",
    "        log_var =  self.encoder[-1].output\n",
    "        z = self.sample_z(mu, log_var)\n",
    "        hidden_data = theano.function([index],\n",
    "                                      outputs = z,\n",
    "                                      givens={self.x: data_set[index : data_size]})\n",
    "        return hidden_data(0)\n",
    "\n",
    "\n",
    "    \"************get data from the output of Autoencoder**********************\"\n",
    "    def get_output_data(self,data_set):\n",
    "        data_size = data_set.get_value().shape[0]\n",
    "        index  = T.lscalar('index')\n",
    "        y_data = theano.function([index],\n",
    "                                      outputs = self.decoder[-1].output,\n",
    "                                      givens={self.x: data_set[index : data_size]})\n",
    "        return y_data(0)\n",
    "\n",
    "    \"******************** Histogram z, z_mu and z_var ************************\"\n",
    "    def Plot_histogram_z(self, train_set, test_set, actual, epoch, path):\n",
    "        z_train    = self.get_hidden_data(train_set)\n",
    "        mu, logvar = self.get_mu_logvar(train_set)\n",
    "\n",
    "        np.savetxt(path + \"Visualize_histogram/\" + \"z_train_\" + str(epoch) + \"_\" + str(self.alpha)+\".csv\", z_train, delimiter=\",\", fmt='%f' )\n",
    "#        np.savetxt(path + \"Visualize_histogram/\" + \"z_mu\" + str(epoch) + \".csv\",  mu, delimiter=\",\", fmt='%f' )\n",
    "#        np.savetxt(path + \"Visualize_histogram/\" + \"z_var\" + str(epoch) + \".csv\", np.exp(logvar), delimiter=\",\", fmt='%f' )\n",
    "\n",
    "#        histogram_z(mu[:,0],             'mu' , self.alpha, epoch, path)\n",
    "#        histogram_z(np.exp(logvar[:,0]), 'var', self.alpha, epoch, path)\n",
    "        histogram_z(z_train[:,0],        'z'  , self.alpha, epoch, path)\n",
    "\n",
    "    \"********************** Standard deviation of z ***************************\"\n",
    "    def Compute_Std(self, train_set, test_set, actual, data_name, path):\n",
    "        z_train    = self.get_hidden_data(train_set)\n",
    "        z_test     = self.get_hidden_data(test_set)\n",
    "\n",
    "        visualize_hidden1(z_train, z_test, actual, data_name, path)\n",
    "        #std on each feature over data\n",
    "        std = np.std(z_train, axis = 0)\n",
    "        np.set_printoptions(precision=6, suppress=True)\n",
    "        print(\"\\n+ Standard Deviation of Hidden data:\")\n",
    "        print(std)\n",
    "\n",
    "    \"********************* Compute AUC on hidden data *************************\"\n",
    "    def Compute_AUC_Hidden(self, train_set, test_set, actual, norm, data_name):\n",
    "        z_train = self.get_hidden_data(train_set)           #get hidden values\n",
    "        z_test  = self.get_hidden_data(test_set)            #get hidden values\n",
    "        y_test  = self.get_output_data(test_set)            #get prediction values\n",
    "        \"Compute performance of classifiers on latent data\"\n",
    "        lof, cen, dis, kde, svm05, svm01 = auc_density(z_train, z_test, actual, norm)\n",
    "        ae                               = auc_AEbased(test_set.get_value(), y_test, actual)\n",
    "        return lof, cen, dis, kde, svm05, svm01, ae\n",
    "\n",
    "    \"**************************************************************************\"\n",
    "    def Save_Hidden_Data(self, train_set, test_set, data_name, path):\n",
    "        z_train = self.get_hidden_data(train_set)      #get hidden values\n",
    "        z_test  = self.get_hidden_data(test_set)       #get hidden values\n",
    "        np.savetxt(path + data_name + \"_train_z.csv\", z_train, delimiter=\",\", fmt='%f' )\n",
    "        np.savetxt(path + data_name + \"_test_z.csv\" ,  z_test, delimiter=\",\", fmt='%f' )\n",
    "\n",
    "    \"**************************************************************************\"\n",
    "    def Save_Hidden_Data_Size(self, train_set, test_set, data_name, size, path):\n",
    "        z_train = self.get_hidden_data(train_set)      #get hidden values\n",
    "        z_test  = self.get_hidden_data(test_set)       #get hidden values\n",
    "        np.savetxt(path + \"data/\"+ data_name + \"_train_z_\" + str(size)+ \".csv\", z_train, delimiter=\",\", fmt='%f' )\n",
    "        np.savetxt(path + \"data/\"+ data_name + \"_test_z_\" + str(size)+ \".csv\" ,  z_test, delimiter=\",\", fmt='%f' )\n",
    "\n",
    "    \"******** Training End-to-End Early-stopping by Downhill Package *********\"\n",
    "    def End2end_Early_stopping(self, numpy_rng, dataset, n_validate, data_name,\n",
    "                               batch_size, end2end_lr, algo, norm, patience, validation):\n",
    "\n",
    "        train_X, test_X, actual = dataset\n",
    "        valid_x = train_X.get_value()[:n_validate]\n",
    "        train_x = train_X.get_value()[n_validate:]\n",
    "        \"for compute tm and vm before optimization process\"\n",
    "        t = theano.shared(numpy.asarray(train_x, dtype=theano.config.floatX), borrow=True)\n",
    "        v = theano.shared(numpy.asarray(valid_x, dtype=theano.config.floatX), borrow=True)\n",
    "\n",
    "        \"Training network by downhill\"\n",
    "        #'adadelta' 'adagrad (default 0.01)' 'adam''esgd' 'nag''rmsprop' 'rprop' 'sgd'\n",
    "        opt = downhill.build(algo = algo, params= self.params, loss = self.end2end_cost, inputs = [self.x])\n",
    "        train = downhill.Dataset(train_x, batch_size = batch_size, rng = numpy_rng)\n",
    "        valid = downhill.Dataset(valid_x, batch_size = len(valid_x), rng = numpy_rng)\n",
    "\n",
    "        \"***** Monitor before optimization *****\"\n",
    "        stop_ep = 0\n",
    "        RE = np.empty([0,3])\n",
    "        monitor = np.empty([0,8])\n",
    "#        LOSS = np.empty([0,3])\n",
    "#        self.Plot_histogram_z(train_X, test_X, actual, 0, path)\n",
    "                                    #AUC before optimization\n",
    "        lof,cen,dis,kde,svm05,svm01,ae = self.Compute_AUC_Hidden(train_X, test_X, actual, norm, data_name)\n",
    "        a = np.column_stack([0, lof, cen, dis, kde, svm05, svm01, ae])\n",
    "        monitor = np.append(monitor, a)\n",
    "                                    #Loss components before optimization\n",
    "\n",
    "#        loss = self.Recon_KL_Loss(t, batch_size)\n",
    "#        LOSS = np.append(LOSS,[stop_ep, loss[0], loss[1]])\n",
    "                                    #Error before optimization\n",
    "        tm1, vm1 = self.Loss_train_valid(t, v)\n",
    "        RE = np.append(RE, np.column_stack([stop_ep, vm1, tm1]))\n",
    "\n",
    "        for tm1, vm1 in opt.iterate(train,                     # 5, 5, 1e-2, 0.9\n",
    "                                  valid,\n",
    "                                  patience = patience,               # 10\n",
    "                                  validate_every = validation,            # 5\n",
    "                                  min_improvement = 1e-3,       # 1e-3\n",
    "                                  #learning_rate =  end2end_lr,  # 1e-4\n",
    "                                  momentum = 0.0,\n",
    "                                  nesterov = False):\n",
    "            stop_ep = stop_ep+1\n",
    "#            loss = self.Recon_KL_Loss(t, batch_size)\n",
    "#            LOSS = np.append(LOSS,[stop_ep, loss[0], loss[1]])\n",
    "#            \"******* Monitor optimization ******\"\n",
    "            if ((stop_ep%200 == 0 ) and (stop_ep > 0)):\n",
    "                #self.Plot_histogram_z(train_X, test_X, actual, stop_ep, path)\n",
    "                lof,cen,dis,kde,svm05,svm01,ae = self.Compute_AUC_Hidden(train_X, test_X, actual, norm, data_name)\n",
    "                a = np.column_stack([stop_ep, lof, cen, dis, kde, svm05, svm01, ae])\n",
    "\n",
    "            monitor = np.append(monitor, a)\n",
    "            re = np.column_stack([stop_ep, vm1['loss'], tm1['loss']])\n",
    "            RE = np.append(RE, re)\n",
    "\n",
    "            if (stop_ep >= 1000):\n",
    "                break\n",
    "\n",
    "        #Plotting AUC and save to csv file\n",
    "        monitor = np.reshape(monitor, (-1,8))\n",
    "#        Plotting_Monitor(monitor, 0.4, 1.0, data_name, path)\n",
    "#        np.savetxt(path + data_name + \"_monitor_auc1.csv\", monitor, delimiter=\",\", fmt='%f' )\n",
    "\n",
    "\n",
    "#        LOSS = np.reshape(LOSS, (-1,3))\n",
    "#        Plotting_Loss_Component(LOSS, RE, 0.0, 0.5, data_name, path)\n",
    "#        np.savetxt(path + data_name + \"_loss_component.csv\", LOSS, delimiter=\",\", fmt='%f' )\n",
    "\n",
    "        RE = np.reshape(RE, (-1,3))\n",
    "#        Plotting_End2End_RE(RE, stop_ep, 0.0, 0.4, data_name, path)\n",
    "#        np.savetxt(path +  data_name + \"_training_error1.csv\", RE, delimiter=\",\", fmt='%f' )\n",
    "\n",
    "        np.set_printoptions(precision=6, suppress=True)\n",
    "        print (\"\\n \",RE[stop_ep])\n",
    "\n",
    "        return RE[stop_ep]\n",
    "\n",
    "\n",
    "\n",
    "def test_SdA(pre_lr=0.01, end2end_lr=1e-4, algo = 'sgd',\n",
    "             dataset=[], data_name = \"WBC\", n_validate = 0, norm = \"maxabs\",\n",
    "             batch_size=10, hidden_sizes = [1,1,1], corruptions = [0.0, 0.0, 0.0],\n",
    "             patience = 1, validation = 1):\n",
    "\n",
    "    numpy_rng = numpy.random.RandomState(89677)     # numpy random generator 89677\n",
    "    train_X, test_X, actual = dataset               # dataset is already normalised\n",
    "\n",
    "    input_size = train_X.get_value().shape[1]       # input size = dimension\n",
    "    train_x    = train_X.get_value()[n_validate:]   # 80% for pre-training, 20% for validation\n",
    "    n_train_batches   = train_x.shape[0]\n",
    "    n_train_batches //= batch_size                  # number of batches for pre-training\n",
    "\n",
    "    # construct the stacked denoising autoencoder class\n",
    "    sda = SdA(numpy_rng = numpy_rng, n_ins = input_size,\n",
    "              hidden_layers_sizes = hidden_sizes)\n",
    "\n",
    "\n",
    "    RE = sda.End2end_Early_stopping(numpy_rng, dataset, n_validate, data_name,\n",
    "                               batch_size, end2end_lr, algo, norm, patience, validation)\n",
    "    return sda, RE\n",
    "\n",
    "\n",
    "def Main_Test():\n",
    "\t\n",
    "    list_data =  [\"PageBlocks\", \"WPBC\", \"PenDigits\", \"GLASS\", \"Shuttle\", \"Arrhythmia\",\\\n",
    "                 \"CTU13_10\", \"CTU13_08\",\"CTU13_09\",\"CTU13_13\",\\\n",
    "                 \"Spambase\", \"UNSW\", \"NSLKDD\", \"InternetAds\"]\n",
    "\n",
    "    norm         = \"maxabs\"            # standard, maxabs[-1,1] or minmax[0,1]\n",
    "    corruptions  = [0.1, 0.1, 0.1]\n",
    "\n",
    "    print (\"+ VAE: 0.05, Group\")\n",
    "    print (\"+ Data: \", list_data)\n",
    "    print (\"+ Scaler: \", norm)\n",
    "\n",
    "    AUC_Hidden = np.empty([0,10])     #store auc of all hidden data\n",
    "    num = 0                           #a counter\n",
    "    for data in list_data:\n",
    "        num = num + 1\n",
    "\n",
    "        h_sizes = hyper_parameters(data)                   #Load hyper-parameters\n",
    "        train_set, test_set, actual = load_data(data)      #load original data\n",
    "\n",
    "        train_X, test_X = normalize_data(train_set, test_set, norm)  #Normalize data\n",
    "\n",
    "        train_X = theano.shared(numpy.asarray(train_X, dtype=theano.config.floatX), borrow=True)\n",
    "        test_X  = theano.shared(numpy.asarray(test_X,  dtype=theano.config.floatX), borrow=True)\n",
    "\n",
    "        datasets = [(train_X), (test_X), (actual)]          #Pack data for training AE\n",
    "\n",
    "        in_dim   = train_set.shape[1]                       #dimension of input data\n",
    "        n_vali   = (int)(train_set.shape[0]/5)              #size of validation set\n",
    "        n_train  = len(train_set) - n_vali                  #size of training set\n",
    "        #batch    = int(n_train/20)                           #Training set will be split training set into 20 batches\n",
    "                                                    #print data information\n",
    "        pat, val, batch, n_batch = stopping_para_vae(n_train)\n",
    "\n",
    "        print (\"\\n\" + str(num) + \".\", data, \"...\" )\n",
    "        print (\" + Hidden Sizes: \",in_dim, h_sizes, \"- Batch_sizes:\", batch)\n",
    "        print (\" + Data: %d (%d train, %d vali) - %d normal, %d anomaly\"\\\n",
    "            %(len(train_set), n_train, n_vali, \\\n",
    "            len(test_set[(actual == 1)]), len(test_set[(actual == 0)])))\n",
    "\n",
    "        print(\" + Patience: %5.0d, Validate: %5.0d,  \\n + Batch size: %5.0d, n batch:%5.0d\"\\\n",
    "             %(pat, val, batch, n_batch))\n",
    "\n",
    "        AUC_RE   = np.empty([0,10])\n",
    "                               #adadelta, 'adagrad' 'adam''esgd' 'nag''rmsprop' 'rprop' 'sgd'\n",
    "        #if (num==1):\n",
    "        sda, re = test_SdA(pre_lr       = 1e-2,            #re = [stop_ep, vm, tm]\n",
    "                                   end2end_lr   = 1e-4,\n",
    "                                   algo         = 'adadelta',\n",
    "                                   dataset      = datasets,\n",
    "                                   data_name    = data,\n",
    "                                   n_validate   = n_vali,\n",
    "                                   norm         = norm,\n",
    "                                   batch_size   = batch,\n",
    "                                   hidden_sizes = h_sizes,\n",
    "                                   corruptions = corruptions,\n",
    "                                   patience     = pat,\n",
    "                                   validation   = val)\n",
    "\n",
    "        #Computer AUC on hidden data\n",
    "        lof,cen,dis,kde,svm05,svm01,ae  = sda.Compute_AUC_Hidden(train_X, test_X, actual, norm, data)\n",
    "        auc_hidden = np.column_stack([batch, re[0], lof, cen, dis, kde, svm05, svm01, ae , 100*re[2]])\n",
    "        AUC_Hidden = np.append(AUC_Hidden, auc_hidden)\n",
    "\n",
    "        #compute standard deviation of z\n",
    "        #sda.Compute_Std(train_X, test_X, actual, data, path)\n",
    "        #save hidden data to files\n",
    "#        sda.Save_Hidden_Data(train_X, test_X, data, path)\n",
    "\n",
    "        #store AUC_input AUC_hidden and RE to AUC_RE for each data\n",
    "#        AUC_RE   = np.append(AUC_RE, auc_hidden)\n",
    "#        AUC_RE   = np.reshape(AUC_RE,(-1,10))\n",
    "#\n",
    "#        print(\"\\n+ AUC input, AUC hidden:\")\n",
    "#        np.set_printoptions(precision=3, suppress=True)\n",
    "#        column_list = [2,3,4,5,6,7,8,9]\n",
    "#        print (AUC_RE[:,column_list])\n",
    "#\n",
    "#        AUC_Hidden = np.append(AUC_Hidden, auc_hidden)\n",
    "        AUC_Hidden  =  np.reshape(AUC_Hidden, (-1, 10))\n",
    "        np.set_printoptions(precision=3, suppress=True)\n",
    "        column_list = [2,3,4,5,6,7,8,9]\n",
    "        print(\"    LOF    CEN    MDIS   KDE   SVM5    SVM1    AE    RE*100\")\n",
    "        print (AUC_Hidden[:,column_list])\n",
    "        print(\"\\n\")\n",
    "        \n",
    "    AUC_Hidden  =  np.reshape(AUC_Hidden, (-1, 10))\n",
    "    np.set_printoptions(precision=3, suppress=True)\n",
    "    column_list = [2,3,4,5,6,7,8,9]\n",
    "    print(\"    LOF    CEN    MDIS   KDE   SVM5    SVM1    AE    RE*100\")\n",
    "    print (AUC_Hidden[:,column_list])\n",
    "\n",
    "#    #store AUC_input and AUC_hidden to AUC_Input, AUC_Hidden\n",
    "#    AUC_Hidden  =  np.reshape(AUC_Hidden, (-1, 10))\n",
    "#    np.savetxt(path +  \"AUC_Hidden.csv\", AUC_Hidden, delimiter=\",\", fmt='%f' )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DAE\n",
      "+ Scaler:  maxabs\n",
      "+ Corruptions:  [0.1, 0.1, 0.1]\n"
     ]
    }
   ],
   "source": [
    "#list_data = [\"Arrhythmia\"]\n",
    "data=\"maiwilab\"\n",
    "norm         = \"maxabs\"                \n",
    "corruptions  = [0.1, 0.1, 0.1]\n",
    "\n",
    "print (\"DAE\")\n",
    "#print (\"+ Data: \", list_data)\n",
    "print (\"+ Scaler: \", norm)\n",
    "print (\"+ Corruptions: \", corruptions)\n",
    "\n",
    "AUC_Hidden = np.empty([0,10])     \n",
    "\n",
    "num = 0\n",
    "\n",
    "num = num + 1\n",
    "h_sizes =[85, 49, 12] #hyper_parameters(data)                   \n",
    "\n",
    "#train_set, test_set, actual = load_data(data)#normal 是1 anormal是0     \n",
    "#train_X, test_X = normalize_data(train_set, test_set, norm) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain=np.load('IDS_x_train.npy')\n",
    "Xtest=np.load('IDS_x_test.npy')\n",
    "total=100000\n",
    "x=np.concatenate((Xtrain,Xtest),axis=0)[:total]\n",
    "\n",
    "\n",
    "Ytrain=np.load('IDS_y_train.npy')\n",
    "Ytest=np.load('IDS_y_test.npy')\n",
    "label=np.concatenate((Ytrain,Ytest),axis=0)[:total]\n",
    "x1=pd.DataFrame(x)\n",
    "#actual=np.array([0 if i==1 else 1 for i in label])\n",
    "x1['label']=label\n",
    "d=x1.values\n",
    "# train_set=x1[:240000].values\n",
    "# test_set=x1[240000:300000].values\n",
    "#train_X, test_X = normalize_data(train_set, test_set, norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = d[~np.isnan(d).any(axis=1)]    #discard the '?' values\n",
    "seed=0\n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(d)\n",
    "\n",
    "dX = d[:,0:-1]              #put data to dX without the last column (labels)\n",
    "dy = d[:,-1].astype(int)                #put label to dy\n",
    "dy = dy > 0\n",
    "\n",
    "                            # dy=True with anomaly labels\n",
    "                            # separate into normal and anomaly\n",
    "dX0 = dX[~dy]               # Normal data \n",
    "dX1 = dX[dy]                # Anomaly data\n",
    "dy0 = dy[~dy]               # Normal label\n",
    "dy1 = dy[dy]                # Anomaly label\n",
    "\n",
    "#print(\"Normal: %d Anomaly %d\" %(len(dX0), len(dX1)))\n",
    "split = 0.8             #split 80% for training, 20% for testing\n",
    "\n",
    "idx0  = int(split * len(dX0))\n",
    "idx1  = int(split * len(dX1))\n",
    "\n",
    "train_set = dX0[:idx0]        # train_X is 80% of the normal class\n",
    "\n",
    "# test set is the other half of the normal class and all of the anomaly class\n",
    "test_set = np.concatenate((dX0[idx0:], dX1[idx1:]))  # 30% of normal and 30% of anomaly\n",
    "test_y = np.concatenate((dy0[idx0:], dy1[idx1:]))  # 30% of normal and 30% of anomaly label\n",
    "#conver test_y into 1 or 0 for computing AUC later\n",
    "actual = (~test_y).astype(np.int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. maiwilab ...\n",
      " + Hidden Sizes:  63 [85, 49, 12] - Batch_sizes: 100\n",
      " + Data: 68776 (55021 train, 13755 vali) -test: 17195 normal, 2806 anomaly\n",
      " + Patience:     4, Validate:     1,  \n",
      " + Batch size:   100, n batch:  550\n"
     ]
    }
   ],
   "source": [
    "train_X, test_X = normalize_data(train_set, test_set, norm)\n",
    "train_X = theano.shared(numpy.asarray(train_X, dtype=theano.config.floatX), borrow=True)\n",
    "test_X  = theano.shared(numpy.asarray(test_X,  dtype=theano.config.floatX), borrow=True)\n",
    "\n",
    "datasets = [(train_X), (test_X), (actual)]          \n",
    "\n",
    "in_dim   = train_set.shape[1]                       \n",
    "n_vali   = (int)(train_set.shape[0]/5)              \n",
    "n_train  = len(train_set) - n_vali                  \n",
    "#batch     = int(n_train/20)                          \n",
    "\n",
    "pat, val, batch, n_batch = stopping_para_vae(n_train)\n",
    "\n",
    "\n",
    "print (\"\\n\" + str(num) + \".\", data, \"...\" )\n",
    "print (\" + Hidden Sizes: \",in_dim, h_sizes, \"- Batch_sizes:\", batch)\n",
    "print (\" + Data: %d (%d train, %d vali) -test: %d normal, %d anomaly\"\\\n",
    "    %(len(train_set), n_train, n_vali, \\\n",
    "    len(test_set[(actual == 1)]), len(test_set[(actual == 0)])))\n",
    "print(\" + Patience: %5.0d, Validate: %5.0d,  \\n + Batch size: %5.0d, n batch:%5.0d\"\\\n",
    "     %(pat, val, batch, n_batch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset0: 551 of 551 mini-batches from (55021, 63)\n",
      "dataset1: 1 of 1 mini-batches from (13755, 63)\n",
      "cen\n",
      "AUC 0.5240309211536696\n",
      "Precision 0.867847117253293\n",
      "Recall 0.7011922070369293\n",
      "f1_score 0.7756690684508492\n",
      "TPR 0.7011922070369293\n",
      "FPR 0.654312188168211\n",
      "KDE AE/DensityBasedOneClassClassifier\n",
      "AUC 0.5222687975772433\n",
      "Precision 0.8666280501049888\n",
      "Recall 0.696074440244257\n",
      "f1_score 0.7720441204928078\n",
      "TPR 0.696074440244257\n",
      "FPR 0.6564504632929437\n",
      "SVM05\n",
      "AUC 0.5219187397420515\n",
      "Precision 0.8664141598077063\n",
      "Recall 0.6917708636231462\n",
      "f1_score 0.7693053938688398\n",
      "TPR 0.6917708636231462\n",
      "FPR 0.6535994297933001\n",
      "SVM01\n",
      "AUC 0.5209797391333364\n",
      "Precision 0.8657339736897056\n",
      "Recall 0.7233498109915674\n",
      "f1_score 0.7881629807996959\n",
      "TPR 0.7233498109915674\n",
      "FPR 0.687455452601568\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "data=\"maiwilab\"\n",
    "AUC_RE   = np.empty([0,10])\n",
    "                   #adadelta, 'adagrad' 'adam''esgd' 'nag''rmsprop' 'rprop' 'sgd'\n",
    "#if (num==1):\n",
    "sda, re = test_SdA(pre_lr       = 1e-2,            #re = [stop_ep, vm, tm]\n",
    "                       end2end_lr   = 1e-4,\n",
    "                       algo         = 'adadelta',\n",
    "                       dataset      = datasets,\n",
    "                       data_name    =data,\n",
    "                       n_validate   = n_vali,\n",
    "                       norm         = norm,\n",
    "                       batch_size   = batch,\n",
    "                       hidden_sizes = h_sizes,\n",
    "                       corruptions = corruptions,\n",
    "                       patience     = pat,\n",
    "                       validation   = val)\n",
    "\n",
    "#Computer AUC on hidden data\n",
    "lof,cen,dis,kde,svm05,svm01,ae  = sda.Compute_AUC_Hidden(train_X, test_X, actual, norm, data)\n",
    "auc_hidden = np.column_stack([batch, re[0], lof, cen, dis, kde, svm05, svm01, ae , 100*re[2]])\n",
    "AUC_Hidden = np.append(AUC_Hidden, auc_hidden)\n",
    "\n",
    "#compute standard deviation of z\n",
    "#sda.Compute_Std(train_X, test_X, actual, data, path)\n",
    "#save hidden data to files\n",
    "#        sda.Save_Hidden_Data(train_X, test_X, data, path)\n",
    "\n",
    "#store AUC_input AUC_hidden and RE to AUC_RE for each data\n",
    "#        AUC_RE   = np.append(AUC_RE, auc_hidden)\n",
    "#        AUC_RE   = np.reshape(AUC_RE,(-1,10))\n",
    "#\n",
    "#        print(\"\\n+ AUC input, AUC hidden:\")\n",
    "#        np.set_printoptions(precision=3, suppress=True)\n",
    "#        column_list = [2,3,4,5,6,7,8,9]\n",
    "#        print (AUC_RE[:,column_list])\n",
    "#\n",
    "#        AUC_Hidden = np.append(AUC_Hidden, auc_hidden)\n",
    "AUC_Hidden  =  np.reshape(AUC_Hidden, (-1, 10))\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "column_list = [2,3,4,5,6,7,8,9]\n",
    "print(\"    LOF    CEN    MDIS   KDE   SVM5    SVM1    AE    RE*100\")\n",
    "print (AUC_Hidden[:,column_list])\n",
    "print(\"\\n\")\n",
    "\n",
    "AUC_Hidden  =  np.reshape(AUC_Hidden, (-1, 10))\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "column_list = [2,3,4,5,6,7,8,9]\n",
    "print(\"    LOF    CEN    MDIS   KDE   SVM5    SVM1    AE    RE*100\")\n",
    "print (AUC_Hidden[:,column_list])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=\"maiwilab\"\n",
    "lof,cen,dis,kde,svm05,svm01,ae  = sda.Compute_AUC_Hidden(train_X, test_X, actual, norm, data)\n",
    "auc_hidden = np.column_stack([batch, re[0], lof, cen, dis, kde, svm05, svm01, ae , 100*re[2]])\n",
    "AUC_Hidden = np.append(AUC_Hidden, auc_hidden)\n",
    "\n",
    "#compute standard deviation of z\n",
    "#sda.Compute_Std(train_X, test_X, actual, data, path)\n",
    "#save hidden data to files\n",
    "#        sda.Save_Hidden_Data(train_X, test_X, data, path)\n",
    "\n",
    "#store AUC_input AUC_hidden and RE to AUC_RE for each data\n",
    "#        AUC_RE   = np.append(AUC_RE, auc_hidden)\n",
    "#        AUC_RE   = np.reshape(AUC_RE,(-1,10))\n",
    "#\n",
    "#        print(\"\\n+ AUC input, AUC hidden:\")\n",
    "#        np.set_printoptions(precision=3, suppress=True)\n",
    "#        column_list = [2,3,4,5,6,7,8,9]\n",
    "#        print (AUC_RE[:,column_list])\n",
    "#\n",
    "#        AUC_Hidden = np.append(AUC_Hidden, auc_hidden)\n",
    "AUC_Hidden  =  np.reshape(AUC_Hidden, (-1, 10))\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "column_list = [2,3,4,5,6,7,8,9]\n",
    "print(\"    LOF    CEN    MDIS   KDE   SVM5    SVM1    AE    RE*100\")\n",
    "print (AUC_Hidden[:,column_list])\n",
    "print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:lr] *",
   "language": "python",
   "name": "conda-env-lr-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
